{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To resolve the comment: <br/>\n",
    "Train the autoencoder + classifier for one sensor, then align the latent space of the other to the known latent space, examine the classifier result. (probably with addition/reduced texture classes) <br/>\n",
    "First case: train BioTac (T4-unspv), then load the model to obtain the \"known latent space\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os,sys\n",
    "CURRENT_TEST_DIR = os.getcwd()\n",
    "sys.path.append(CURRENT_TEST_DIR + \"/../new_iteration/\")\n",
    "import pickle\n",
    "import argparse\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from datetime import datetime\n",
    "\n",
    "from vrae.vrae import VRAEC\n",
    "from vrae.utils import *\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.utils import data as data2\n",
    "from torch.utils.data.dataset import random_split\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "from datetime import datetime\n",
    "from tas_utils_bs import get_trainValLoader, get_testLoader\n",
    "\n",
    "from torchsummary import summary\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dummy class to replace argparser\n",
    "class Args:\n",
    "  kfold = 0\n",
    "  reduction = 1\n",
    "  cuda = '0'\n",
    "\n",
    "args=Args()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "load 0 kfold number, reduce data to 1 folds, put to cuda:0\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<torch._C.Generator at 0x7fd98fdc8930>"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(\"load {} kfold number, reduce data to {} folds, put to cuda:{}\".format(args.kfold, args.reduction, args.cuda))\n",
    "\n",
    "# Set hyper params\n",
    "kfold_number = args.kfold\n",
    "data_reduction_ratio = args.reduction\n",
    "shuffle = True\n",
    "num_class = 20\n",
    "sequence_length_B = 400\n",
    "sequence_length_I = 75\n",
    "number_of_features_B = 19\n",
    "number_of_features_I = 60\n",
    "\n",
    "hidden_size = 90\n",
    "hidden_layer_depth = 1\n",
    "latent_length = 40\n",
    "batch_size = 32\n",
    "learning_rate = 0.001\n",
    "n_epochs = 200\n",
    "\n",
    "\n",
    "dropout_rate = 0.2\n",
    "cuda = True # options: True, False\n",
    "print_every=30\n",
    "clip = True # options: True, False\n",
    "max_grad_norm=5\n",
    "header_B = None\n",
    "header_I = \"CNN\"\n",
    "\n",
    "w_mse = 1 # mse between latent vectors\n",
    "w_rB = 0.05 # recon for B\n",
    "w_rI = 0.01 # recon for I\n",
    "w_cB = 1 # classify for B\n",
    "w_cI = 1 # classify for I\n",
    "\n",
    "\n",
    "np.random.seed(1)\n",
    "torch.manual_seed(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading data...\n"
     ]
    }
   ],
   "source": [
    "# Load data\n",
    "data_dir = '../../new_data_folder/'\n",
    "\n",
    "logDir = 'models_and_stats/'\n",
    "# pretrained model\n",
    "model_name_B = \"T5-half-spv-B\"\n",
    "# new model \n",
    "model_name_I = \"IcubCNN_ae_1_rm_0_wrI_0.01_wC_1_0\"\n",
    "if torch.cuda.is_available():\n",
    "    device = torch.device(\"cuda:{}\".format(args.cuda))\n",
    "else:\n",
    "    device = torch.device('cpu')\n",
    "print(\"Loading data...\")\n",
    "\n",
    "# manually change the test data\n",
    "kfold_number = 1\n",
    "train_loader, val_loader, train_dataset, val_dataset = get_trainValLoader(data_dir, k=kfold_number, spike_ready=False, batch_size=batch_size, shuffle=shuffle)\n",
    "test_loader, test_dataset = get_testLoader(data_dir, spike_ready=False, batch_size=batch_size, shuffle=shuffle)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "load model I from\n",
      "IcubCNN_ae_1_rm_0_wrI_0.01_wC_1_0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ruihan/anaconda3/envs/opencv/lib/python3.8/site-packages/torch/nn/modules/rnn.py:47: UserWarning:\n",
      "\n",
      "dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.2 and num_layers=1\n",
      "\n"
     ]
    }
   ],
   "source": [
    "model_B = VRAEC(num_class=num_class,\n",
    "            sequence_length=sequence_length_B,\n",
    "            number_of_features = number_of_features_B,\n",
    "            hidden_size = hidden_size, \n",
    "            hidden_layer_depth = hidden_layer_depth,\n",
    "            latent_length = latent_length,\n",
    "            batch_size = batch_size,\n",
    "            learning_rate = learning_rate,\n",
    "            n_epochs = n_epochs,\n",
    "            dropout_rate = dropout_rate, \n",
    "            cuda = cuda,\n",
    "            print_every=print_every, \n",
    "            clip=clip, \n",
    "            max_grad_norm=max_grad_norm,\n",
    "            dload = logDir,\n",
    "            model_name=model_name_B,\n",
    "            header=header_B,\n",
    "            device = device)\n",
    "model_B.to(device)\n",
    "\n",
    "\n",
    "model_I_pretrained = VRAEC(num_class=num_class,\n",
    "            sequence_length=sequence_length_I,\n",
    "            number_of_features = number_of_features_I,\n",
    "            hidden_size = hidden_size, \n",
    "            hidden_layer_depth = hidden_layer_depth,\n",
    "            latent_length = latent_length,\n",
    "            batch_size = batch_size,\n",
    "            learning_rate = learning_rate,\n",
    "            n_epochs = n_epochs,\n",
    "            dropout_rate = dropout_rate, \n",
    "            cuda = cuda,\n",
    "            print_every=print_every, \n",
    "            clip=clip, \n",
    "            max_grad_norm=max_grad_norm,\n",
    "            dload = logDir,\n",
    "            model_name=model_name_I,\n",
    "            header=header_I,\n",
    "            device = device)\n",
    "model_I_pretrained.to(device)\n",
    "\n",
    "model_I_pretrained_dir = logDir+model_name_I+'.pt'\n",
    "\n",
    "if device  == torch.device('cpu'):\n",
    "    model_I_pretrained.load_state_dict(torch.load(model_I_pretrained_dir, map_location=torch.device('cpu')))\n",
    "else:\n",
    "    model_I_pretrained.load_state_dict(torch.load(model_I_pretrained_dir))\n",
    "\n",
    "model_I_pretrained.eval()\n",
    "\n",
    "print(\"load model I from\")\n",
    "print(model_name_I)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = nn.CrossEntropyLoss()\n",
    "optimB = optim.Adam(model_B.parameters(), lr=learning_rate)\n",
    "cl_loss_fn = nn.NLLLoss()\n",
    "recon_loss_fn = nn.MSELoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "last batch training: LB: 19.47, LC: 7.44, recon_B 335.80, cl_B 2.68\n",
      "Epoch 0: Loss: lc 7.550,  train_B0.687, val_B 0.665, \n",
      "\t Acc: train_B 0.155, val_B 0.443, train_I 0.977, val_I 1.000\n",
      "--------------------\n",
      "Saving model at 0 epoch to models_and_stats/T5-half-spv-B.pt\n",
      "last batch training: LB: 16.59, LC: 7.76, recon_B 288.65, cl_B 2.16\n",
      "Epoch 1: Loss: lc 7.541,  train_B0.588, val_B 0.540, \n",
      "\t Acc: train_B 0.472, val_B 0.490, train_I 0.977, val_I 1.000\n",
      "--------------------\n",
      "Saving model at 1 epoch to models_and_stats/T5-half-spv-B.pt\n",
      "last batch training: LB: 15.79, LC: 8.02, recon_B 283.30, cl_B 1.63\n",
      "Epoch 2: Loss: lc 8.431,  train_B0.499, val_B 0.462, \n",
      "\t Acc: train_B 0.535, val_B 0.641, train_I 0.979, val_I 1.000\n",
      "--------------------\n",
      "Saving model at 2 epoch to models_and_stats/T5-half-spv-B.pt\n",
      "last batch training: LB: 12.90, LC: 8.96, recon_B 232.68, cl_B 1.26\n",
      "Epoch 3: Loss: lc 9.178,  train_B0.433, val_B 0.415, \n",
      "\t Acc: train_B 0.604, val_B 0.688, train_I 0.977, val_I 1.000\n",
      "--------------------\n",
      "Saving model at 3 epoch to models_and_stats/T5-half-spv-B.pt\n",
      "last batch training: LB: 11.89, LC: 9.06, recon_B 212.37, cl_B 1.28\n",
      "Epoch 4: Loss: lc 9.067,  train_B0.395, val_B 0.380, \n",
      "\t Acc: train_B 0.668, val_B 0.693, train_I 0.977, val_I 1.000\n",
      "--------------------\n",
      "Saving model at 4 epoch to models_and_stats/T5-half-spv-B.pt\n",
      "last batch training: LB: 10.93, LC: 10.66, recon_B 195.49, cl_B 1.16\n",
      "Epoch 5: Loss: lc 9.751,  train_B0.367, val_B 0.359, \n",
      "\t Acc: train_B 0.696, val_B 0.708, train_I 0.979, val_I 1.000\n",
      "--------------------\n",
      "Saving model at 5 epoch to models_and_stats/T5-half-spv-B.pt\n",
      "last batch training: LB: 11.47, LC: 9.57, recon_B 211.35, cl_B 0.90\n",
      "Epoch 6: Loss: lc 9.902,  train_B0.339, val_B 0.332, \n",
      "\t Acc: train_B 0.757, val_B 0.807, train_I 0.979, val_I 1.000\n",
      "--------------------\n",
      "Saving model at 6 epoch to models_and_stats/T5-half-spv-B.pt\n",
      "last batch training: LB: 10.01, LC: 10.58, recon_B 178.79, cl_B 1.07\n",
      "Epoch 7: Loss: lc 9.972,  train_B0.318, val_B 0.316, \n",
      "\t Acc: train_B 0.786, val_B 0.781, train_I 0.981, val_I 1.000\n",
      "--------------------\n",
      "last batch training: LB: 9.82, LC: 10.60, recon_B 179.38, cl_B 0.85\n",
      "Epoch 8: Loss: lc 10.394,  train_B0.304, val_B 0.298, \n",
      "\t Acc: train_B 0.771, val_B 0.812, train_I 0.981, val_I 1.000\n",
      "--------------------\n",
      "Saving model at 8 epoch to models_and_stats/T5-half-spv-B.pt\n",
      "last batch training: LB: 7.72, LC: 10.09, recon_B 138.32, cl_B 0.81\n",
      "Epoch 9: Loss: lc 9.975,  train_B0.285, val_B 0.286, \n",
      "\t Acc: train_B 0.781, val_B 0.823, train_I 0.977, val_I 1.000\n",
      "--------------------\n",
      "Saving model at 9 epoch to models_and_stats/T5-half-spv-B.pt\n",
      "last batch training: LB: 7.67, LC: 10.02, recon_B 138.55, cl_B 0.74\n",
      "Epoch 10: Loss: lc 10.148,  train_B0.272, val_B 0.269, \n",
      "\t Acc: train_B 0.802, val_B 0.828, train_I 0.977, val_I 1.000\n",
      "--------------------\n",
      "Saving model at 10 epoch to models_and_stats/T5-half-spv-B.pt\n",
      "last batch training: LB: 8.86, LC: 10.52, recon_B 164.32, cl_B 0.65\n",
      "Epoch 11: Loss: lc 11.149,  train_B0.260, val_B 0.260, \n",
      "\t Acc: train_B 0.816, val_B 0.818, train_I 0.977, val_I 1.000\n",
      "--------------------\n",
      "last batch training: LB: 6.70, LC: 9.99, recon_B 124.33, cl_B 0.48\n",
      "Epoch 12: Loss: lc 10.347,  train_B0.248, val_B 0.244, \n",
      "\t Acc: train_B 0.812, val_B 0.885, train_I 0.977, val_I 1.000\n",
      "--------------------\n",
      "Saving model at 12 epoch to models_and_stats/T5-half-spv-B.pt\n",
      "last batch training: LB: 6.67, LC: 10.14, recon_B 121.13, cl_B 0.61\n",
      "Epoch 13: Loss: lc 9.609,  train_B0.236, val_B 0.232, \n",
      "\t Acc: train_B 0.806, val_B 0.760, train_I 0.977, val_I 1.000\n",
      "--------------------\n",
      "last batch training: LB: 8.76, LC: 10.99, recon_B 167.91, cl_B 0.36\n",
      "Epoch 14: Loss: lc 9.524,  train_B0.228, val_B 0.231, \n",
      "\t Acc: train_B 0.783, val_B 0.807, train_I 0.981, val_I 1.000\n",
      "--------------------\n",
      "last batch training: LB: 7.32, LC: 11.11, recon_B 133.32, cl_B 0.66\n",
      "Epoch 15: Loss: lc 10.937,  train_B0.217, val_B 0.222, \n",
      "\t Acc: train_B 0.809, val_B 0.823, train_I 0.981, val_I 1.000\n",
      "--------------------\n",
      "last batch training: LB: 6.87, LC: 10.36, recon_B 121.23, cl_B 0.81\n",
      "Epoch 16: Loss: lc 10.036,  train_B0.212, val_B 0.214, \n",
      "\t Acc: train_B 0.826, val_B 0.807, train_I 0.977, val_I 1.000\n",
      "--------------------\n",
      "last batch training: LB: 5.44, LC: 9.68, recon_B 93.48, cl_B 0.76\n",
      "Epoch 17: Loss: lc 9.480,  train_B0.204, val_B 0.203, \n",
      "\t Acc: train_B 0.854, val_B 0.844, train_I 0.979, val_I 1.000\n",
      "--------------------\n",
      "last batch training: LB: 6.48, LC: 10.59, recon_B 119.26, cl_B 0.52\n",
      "Epoch 18: Loss: lc 10.847,  train_B0.195, val_B 0.196, \n",
      "\t Acc: train_B 0.840, val_B 0.880, train_I 0.977, val_I 1.000\n",
      "--------------------\n",
      "last batch training: LB: 6.74, LC: 10.46, recon_B 125.32, cl_B 0.48\n",
      "Epoch 19: Loss: lc 10.259,  train_B0.189, val_B 0.193, \n",
      "\t Acc: train_B 0.865, val_B 0.844, train_I 0.977, val_I 1.000\n",
      "--------------------\n",
      "Saving model at 31 epoch to models_and_stats/T5-half-spv-B.pt\n",
      "Saving model at 33 epoch to models_and_stats/T5-half-spv-B.pt\n",
      "Saving model at 36 epoch to models_and_stats/T5-half-spv-B.pt\n",
      "Saving model at 38 epoch to models_and_stats/T5-half-spv-B.pt\n",
      "Saving model at 42 epoch to models_and_stats/T5-half-spv-B.pt\n",
      "Saving model at 51 epoch to models_and_stats/T5-half-spv-B.pt\n",
      "Saving model at 52 epoch to models_and_stats/T5-half-spv-B.pt\n",
      "Saving model at 57 epoch to models_and_stats/T5-half-spv-B.pt\n",
      "Saving model at 65 epoch to models_and_stats/T5-half-spv-B.pt\n",
      "Saving model at 80 epoch to models_and_stats/T5-half-spv-B.pt\n",
      "Saving model at 105 epoch to models_and_stats/T5-half-spv-B.pt\n",
      "Saving model at 168 epoch to models_and_stats/T5-half-spv-B.pt\n",
      "Saving model at 176 epoch to models_and_stats/T5-half-spv-B.pt\n",
      "Saving model at 178 epoch to models_and_stats/T5-half-spv-B.pt\n",
      "Saving model at 184 epoch to models_and_stats/T5-half-spv-B.pt\n",
      "Saving model at 196 epoch to models_and_stats/T5-half-spv-B.pt\n",
      "RAE training takes time 0:31:41.685810\n"
     ]
    }
   ],
   "source": [
    "# one stage training: with recon_loss and mse_loss\n",
    "training_start=datetime.now()\n",
    "\n",
    "epoch_train_loss_B = []\n",
    "epoch_train_acc_B = []\n",
    "epoch_val_loss_B = []\n",
    "epoch_val_acc_B = []\n",
    "max_val_acc_B = 0\n",
    "\n",
    "# epoch_train_loss_I = []\n",
    "epoch_train_acc_I = []\n",
    "# epoch_val_loss_I = []\n",
    "epoch_val_acc_I = []\n",
    "max_val_acc_I = 0\n",
    "\n",
    "epoch_val_loss_tot = []\n",
    "epoch_train_loss_tot = []\n",
    "\n",
    "for epoch in range(n_epochs):\n",
    "\n",
    "    # TRAIN\n",
    "    model_B.train()\n",
    "    model_I_pretrained.eval()\n",
    "#     model_I.train()\n",
    "\n",
    "    correct_B = 0\n",
    "    train_loss_B = 0\n",
    "    correct_I = 0\n",
    "    train_loss_I = 0\n",
    "    train_loss_tot = 0\n",
    "    train_num = 0\n",
    "            \n",
    "    for i, (XI, XB,  y) in enumerate(train_loader):\n",
    "        XI, XB, y = XI.to(device), XB.to(device), y.long().to(device)\n",
    "        \n",
    "        if XI.size()[0] != batch_size:\n",
    "#             print(\"batch {} size {} < {}, skip\".format(i, x.size()[0], batch_size))\n",
    "            break\n",
    "\n",
    "        train_num += XI.size(0)\n",
    "        \n",
    "        # train model_B\n",
    "        optimB.zero_grad()  \n",
    "        x_decoded_B, latent_B, output = model_B(XB)\n",
    "        \n",
    "        # construct loss function\n",
    "        recon_loss_B = recon_loss_fn(x_decoded_B, XB)\n",
    "        cl_loss_B = cl_loss_fn(output, y)\n",
    "        loss_B = w_rB*recon_loss_B + w_cB*cl_loss_B\n",
    "        \n",
    "        # compute classification acc\n",
    "        pred = output.data.max(1, keepdim=True)[1]  # get the index of the max log-probability\n",
    "        correct_B += pred.eq(y.data.view_as(pred)).long().cpu().sum().item()\n",
    "        # accumulator\n",
    "        train_loss_B += loss_B.item()\n",
    "        \n",
    "        \n",
    "        # train modelI\n",
    "#         optimI.zero_grad()  \n",
    "        x_decoded_I, latent_I, output = model_I_pretrained(XI)\n",
    "        \n",
    "        # construct loss function\n",
    "#         recon_loss_I = recon_loss_fn(x_decoded_I, XI)\n",
    "#         cl_loss_I = cl_loss_fn(output, y)\n",
    "#         loss_I = w_rI*recon_loss_I # + w_cI*cl_loss_I\n",
    "\n",
    "        # compute classification acc\n",
    "        pred = output.data.max(1, keepdim=True)[1]  # get the index of the max log-probability\n",
    "        correct_I += pred.eq(y.data.view_as(pred)).long().cpu().sum().item()\n",
    "        # accumulator\n",
    "#         train_loss_I += loss_I.item()\n",
    "        \n",
    "        loss_C = F.mse_loss(latent_B, latent_I)\n",
    "        if epoch<20:\n",
    "            loss = loss_B\n",
    "        else:\n",
    "            loss =  loss_B + w_mse*loss_C \n",
    "        \n",
    "#         if epoch < 20:\n",
    "# #             loss_I.backward()\n",
    "#             loss_B.backward()\n",
    "#         else:\n",
    "#             loss.backward()\n",
    "        loss.backward()\n",
    "        \n",
    "        train_loss_tot += loss.item()\n",
    "        \n",
    "        optimB.step() \n",
    "#         optimI.step() \n",
    "    \n",
    "    if epoch < 20 or epoch%200 == 0:\n",
    "        print(\"last batch training: LB: {:.2f}, LC: {:.2f}, recon_B {:.2f}, cl_B {:.2f}\"\\\n",
    "              .format(loss_B, loss_C, recon_loss_B, cl_loss_B))\n",
    "    \n",
    "    \n",
    "    # fill stats\n",
    "    train_accuracy_B = correct_B / train_num # len(train_loader.dataset)\n",
    "    train_loss_B /= train_num #len(train_loader.dataset)\n",
    "    epoch_train_loss_B.append(train_loss_B)\n",
    "    epoch_train_acc_B.append(train_accuracy_B) \n",
    "    \n",
    "    train_accuracy_I = correct_I / train_num # len(train_loader.dataset)\n",
    "#     train_loss_I /= train_num #len(train_loader.dataset)\n",
    "#     epoch_train_loss_I.append(train_loss_I)\n",
    "    epoch_train_acc_I.append(train_accuracy_I) \n",
    "    \n",
    "    epoch_train_loss_tot.append(train_loss_tot)\n",
    "    \n",
    "\n",
    "    # VALIDATION\n",
    "    model_B.eval()\n",
    "    model_I_pretrained.eval()\n",
    "\n",
    "    correct_B = 0\n",
    "    val_loss_B = 0\n",
    "    correct_I = 0\n",
    "    val_loss_I = 0\n",
    "    val_loss_tot = 0\n",
    "    val_num = 0\n",
    "\n",
    "    for i, (XI, XB,  y) in enumerate(val_loader):\n",
    "        XI, XB, y = XI.to(device), XB.to(device), y.long().to(device)\n",
    "        \n",
    "        if XI.size()[0] != batch_size:\n",
    "#             print(\"batch {} size {} < {}, skip\".format(i, x.size()[0], batch_size))\n",
    "            break\n",
    "\n",
    "        val_num += XI.size(0)\n",
    "        \n",
    "        # eval model_B\n",
    "        x_decoded_B, latent_B, output = model_B(XB)\n",
    "        # construct loss function\n",
    "        recon_loss_B = recon_loss_fn(x_decoded_B, XB)\n",
    "        cl_loss_B = cl_loss_fn(output, y)\n",
    "        loss_B = w_rB*recon_loss_B + w_cB*cl_loss_B\n",
    "       \n",
    "        # compute classification acc\n",
    "        pred = output.data.max(1, keepdim=True)[1]  # get the index of the max log-probability\n",
    "        correct_B += pred.eq(y.data.view_as(pred)).long().cpu().sum().item()\n",
    "        # accumulator\n",
    "        val_loss_B += loss_B.item()\n",
    "        \n",
    "        \n",
    "        # eval modelI \n",
    "        x_decoded_I, latent_I, output = model_I_pretrained(XI)\n",
    "        # construct loss function\n",
    "#         recon_loss_I = recon_loss_fn(x_decoded_I, XI)\n",
    "#         cl_loss_I = cl_loss_fn(output, y)\n",
    "#         loss_I = w_rI*recon_loss_I # + w_cI*cl_loss_I\n",
    "\n",
    "        # compute classification acc\n",
    "        pred = output.data.max(1, keepdim=True)[1]  # get the index of the max log-probability\n",
    "        correct_I += pred.eq(y.data.view_as(pred)).long().cpu().sum().item()\n",
    "        # accumulator\n",
    "#         val_loss_I += loss_I.item()\n",
    "        \n",
    "        loss_C = F.mse_loss(latent_B, latent_I)\n",
    "        loss =  loss_B + w_mse*loss_C\n",
    "        val_loss_tot += loss.item()\n",
    "\n",
    "    # fill stats\n",
    "    val_accuracy_B = correct_B / val_num # len(train_loader.dataset)\n",
    "    val_loss_B /= val_num #len(train_loader.dataset)\n",
    "    epoch_val_loss_B.append(val_loss_B)\n",
    "    epoch_val_acc_B.append(val_accuracy_B) \n",
    "    \n",
    "    val_accuracy_I = correct_I / val_num # len(train_loader.dataset)\n",
    "#     val_loss_I /= val_num #len(train_loader.dataset)\n",
    "#     epoch_val_loss_I.append(val_loss_I)\n",
    "    epoch_val_acc_I.append(val_accuracy_I) \n",
    "    epoch_val_loss_tot.append(val_loss_tot)\n",
    "    \n",
    "    if epoch < 20 or epoch%200 == 0:\n",
    "#         print(\"train_num {}, val_num {}\".format(train_num, val_num))\n",
    "        print(\"Epoch {}: Loss: lc {:.3f},  train_B{:.3f}, val_B {:.3f}, \\n\\t Acc: train_B {:.3f}, val_B {:.3f}, train_I {:.3f}, val_I {:.3f}\"\\\n",
    "              .format(epoch, loss_C, train_loss_B, val_loss_B, train_accuracy_B, val_accuracy_B, train_accuracy_I, val_accuracy_I))\n",
    "        print(\"-\"*20)\n",
    "\n",
    "    # choose model\n",
    "    # TODO: not save at the same time, may have bad common representation\n",
    "    if max_val_acc_B <= val_accuracy_B:\n",
    "        model_dir = logDir + model_name_B + '.pt'\n",
    "        print(\"Saving model at {} epoch to {}\".format(epoch, model_dir))\n",
    "        max_val_acc_B = val_accuracy_B\n",
    "        torch.save(model_B.state_dict(), model_dir)\n",
    "\n",
    "#     if max_val_acc_I <= val_accuracy_I:\n",
    "#         model_dir = logDir + model_name_I + '.pt'\n",
    "#         print(\"Saving model at {} epoch to {}\".format(epoch, model_dir))\n",
    "#         max_val_acc_I = val_accuracy_I\n",
    "#         torch.save(model_I.state_dict(), model_dir)\n",
    "    \n",
    "\n",
    "training_end =  datetime.now()\n",
    "training_time = training_end -training_start \n",
    "print(\"RAE training takes time {}\".format(training_time)) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "VRAE(n_epochs=200,batch_size=32,cuda=False)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_B.is_fitted = True\n",
    "model_I_pretrained.is_fitted = True\n",
    "\n",
    "model_B.eval()\n",
    "model_I_pretrained.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "VRAE(n_epochs=200,batch_size=32,cuda=False)\n"
     ]
    }
   ],
   "source": [
    "# copy the classifier from B to I to examine the classification result\n",
    "classifier_keys = ['classifier.0.weight', 'classifier.0.bias']\n",
    "classifier_dict_B = {k: v for k, v in model_B.state_dict().items() if k in classifier_keys}\n",
    "classifier_dict_I = {k: v for k, v in model_I_pretrained.state_dict().items() if k in classifier_keys}\n",
    "ae_dict_B = {k: v for k, v in model_B.state_dict().items() if k not in classifier_keys}\n",
    "ae_dict_I = {k: v for k, v in model_I_pretrained.state_dict().items() if k not in classifier_keys}\n",
    "\n",
    "newB_dict = model_B.state_dict()\n",
    "newB_dict.update(ae_dict_B)\n",
    "# overwrite classifer for new models\n",
    "newB_dict.update(classifier_dict_I)\n",
    "# load the new state_dict\n",
    "model_B.load_state_dict(newB_dict)\n",
    "model_B.to(device)\n",
    "model_B.eval()\n",
    "print(model_B)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test accuracy for 1 fold 192 samples: B 0.9479166666666666, I 0.9270833333333334\n"
     ]
    }
   ],
   "source": [
    "# TEST\n",
    "correct_B = 0\n",
    "correct_I = 0\n",
    "test_num = 0\n",
    "\n",
    "for i, (XI, XB,  y) in enumerate(test_loader):\n",
    "    XI, XB, y = XI.to(device), XB.to(device), y.long().to(device)\n",
    "\n",
    "    if XI.size()[0] != batch_size:\n",
    "#             print(\"batch {} size {} < {}, skip\".format(i, x.size()[0], batch_size))\n",
    "        break\n",
    "\n",
    "    test_num += XI.size(0)\n",
    "\n",
    "    # test model_B\n",
    "    x_decoded_B, latent_B, output = model_B(XB)\n",
    "\n",
    "    # compute classification acc\n",
    "    pred = output.data.max(1, keepdim=True)[1]  # get the index of the max log-probability\n",
    "    correct_B += pred.eq(y.data.view_as(pred)).long().cpu().sum().item()\n",
    "\n",
    "    \n",
    "    # test modelI \n",
    "    x_decoded_I, latent_I, output = model_I_pretrained(XI)\n",
    "\n",
    "    # compute classification acc\n",
    "    pred = output.data.max(1, keepdim=True)[1]  # get the index of the max log-probability\n",
    "    correct_I += pred.eq(y.data.view_as(pred)).long().cpu().sum().item()\n",
    "\n",
    "test_acc_B = correct_B/test_num\n",
    "test_acc_I = correct_I/test_num\n",
    "print('Test accuracy for {} fold {} samples: B {}, I {}'.format(str(kfold_number),test_num, test_acc_B, test_acc_I))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save stats\n",
    "# results_dict = {\"epoch_train_loss_B\": epoch_train_loss_B,\n",
    "#                 \"epoch_train_loss_I\": epoch_train_loss_I,\n",
    "#                 \"epoch_val_loss_B\": epoch_val_loss_B,\n",
    "#                 \"epoch_val_loss_I\": epoch_val_loss_I,\n",
    "#                 \"epoch_train_acc_B\": epoch_train_acc_B,\n",
    "#                 \"epoch_train_acc_I\": epoch_train_acc_I,\n",
    "#                 \"epoch_val_acc_B\": epoch_val_acc_B,\n",
    "#                 \"epoch_val_acc_I\": epoch_val_acc_I,\n",
    "#                 \"test_acc\": [test_acc_B, test_acc_I]}\n",
    "\n",
    "# since we examine the classifier at the last step, the train acc and val acc do not matter\n",
    "results_dict = {\"epoch_train_loss_B\": epoch_train_loss_B,\n",
    "                \"epoch_val_loss_B\": epoch_val_loss_B,\n",
    "                \"epoch_train_loss_tot\": epoch_train_loss_tot,\n",
    "                \"epoch_val_loss_tot\": epoch_val_loss_tot,\n",
    "#                 \"epoch_train_acc_B\": epoch_train_acc_B,\n",
    "#                 \"epoch_train_acc_I\": epoch_train_acc_I,\n",
    "#                 \"epoch_val_acc_B\": epoch_val_acc_B,\n",
    "#                 \"epoch_val_acc_I\": epoch_val_acc_I,\n",
    "                \"test_acc\": [test_acc_B, test_acc_I]}\n",
    "dict_name = \"T5_BT19Icub_joint_ae_wrB_{}_wcB_{}_wrI_{}_wcI_{}_wC_{}_{}_trainB.pkl\".format(w_rB, w_cB, w_rI, w_cI, w_mse, str(kfold_number))\n",
    "pickle.dump(results_dict, open(logDir + dict_name, 'wb'))\n",
    "print(\"dump results dict to {}\".format(dict_name))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot loss\n",
    "assert n_epochs == len(epoch_train_loss_tot), \"different epoch length {} {}\".format(n_epochs, len(epoch_train_loss_tot))\n",
    "fig, ax = plt.subplots(figsize=(15, 7))\n",
    "ax.plot(np.arange(n_epochs), epoch_train_loss_tot, label=\"epoch_train_loss_tot\")\n",
    "ax.set_xlabel('epoch')\n",
    "ax.set_ylabel('acc')\n",
    "ax.grid(True)\n",
    "plt.legend(loc='upper right')\n",
    "figname = logDir + model_name_B + \"_train_loss_tot.png\"\n",
    "plt.savefig(figname)\n",
    "plt.show()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
