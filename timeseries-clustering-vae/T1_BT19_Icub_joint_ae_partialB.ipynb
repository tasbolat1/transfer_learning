{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "python T1_BT19_Icub_joint_ae_partialB.py -k 0 -c 0 -r 1 -rr 3\n",
    "\n",
    "Note: still use one-stage training with both recon loss and classification loss\n",
    "'''\n",
    "# Import\n",
    "\n",
    "import os,sys\n",
    "CURRENT_TEST_DIR = os.getcwd()\n",
    "sys.path.append(CURRENT_TEST_DIR + \"/../new_iteration/\")\n",
    "import pickle\n",
    "import argparse\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from datetime import datetime\n",
    "\n",
    "from vrae.vrae import VRAEC\n",
    "from vrae.utils import *\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.utils import data as data2\n",
    "from torch.utils.data.dataset import random_split\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "from datetime import datetime\n",
    "from tas_utils_bs import get_trainValLoader, get_testLoader\n",
    "import plotly\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parse argument\n",
    "parser = argparse.ArgumentParser()\n",
    "parser.add_argument(\"-k\", \"--kfold\", type=int, default=0, help=\"kfold_number for loading data\")\n",
    "parser.add_argument(\"-r\", \"--reduction\", type=int, default=1, help=\"data reduction ratio for partial training\")\n",
    "parser.add_argument(\"-c\", \"--cuda\", default=0, help=\"index of cuda gpu to use\")\n",
    "parser.add_argument(\"-rr\", \"--removal\", type=int, default=0, help=\"number of batches removed from training\")\n",
    "args = parser.parse_args()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # dummy class to replace argparser\n",
    "# class Args:\n",
    "#   kfold = 0\n",
    "#   reduction = 1\n",
    "#   cuda = '0'\n",
    "#   removal = 2\n",
    "\n",
    "# args=Args()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "load 0 kfold number, remove 2 batch of training data, put to cuda:0\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<torch._C.Generator at 0x7f0cc76e4550>"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "if args.reduction != 1:\n",
    "    print(\"load {} kfold number, reduce data to {} folds, put to cuda:{}\".format(args.kfold, args.reduction, args.cuda))\n",
    "    assert args.removal == 0, \"removal must be 0 for kfold reduction\"\n",
    "elif args.removal != 0:\n",
    "    print(\"load {} kfold number, remove {} batch of training data, put to cuda:{}\".format(args.kfold, args.removal, args.cuda))\n",
    "else:\n",
    "    print(\"load {} kfold number, put to cuda:{}, train with full data\".format(args.kfold, args.cuda))\n",
    "\n",
    "# Set hyper params\n",
    "kfold_number = args.kfold\n",
    "data_reduction_ratio = args.reduction\n",
    "removal = args.removal\n",
    "shuffle = False\n",
    "num_class = 20\n",
    "sequence_length_B = 400\n",
    "sequence_length_I = 75\n",
    "number_of_features_B = 19\n",
    "number_of_features_I = 60\n",
    "\n",
    "hidden_size = 90\n",
    "hidden_layer_depth = 1\n",
    "latent_length = 40\n",
    "batch_size = 32\n",
    "learning_rate = 0.0005\n",
    "n_epochs = 2000\n",
    "\n",
    "\n",
    "dropout_rate = 0.2\n",
    "cuda = True # options: True, False\n",
    "print_every=30\n",
    "clip = True # options: True, False\n",
    "max_grad_norm=5\n",
    "header_B = None\n",
    "header_I = \"CNN\"\n",
    "\n",
    "w_mse = 1 # mse between latent vectors\n",
    "w_rB = 0.01 # recon for B\n",
    "w_rI = 0.01 # recon for I\n",
    "w_cB = 1 # classify for B\n",
    "w_cI = 1 # classify for I\n",
    "\n",
    "\n",
    "np.random.seed(1)\n",
    "torch.manual_seed(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load data and preprocess"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading data...\n"
     ]
    }
   ],
   "source": [
    "# Load data\n",
    "data_dir = '../../new_data_folder/'\n",
    "\n",
    "logDir = 'models_and_stat/'\n",
    "# new model\n",
    "model_name_B = 'BT19_joint_ae_wrB_{}_wcB_{}_wrI_{}_wcI_{}_wC_{}_reductB_{}_rm_{}_{}'.format(w_rB,w_cB, w_rI, w_cI, w_mse, data_reduction_ratio, removal, str(kfold_number))\n",
    "model_name_I = 'IcubCNN_joint_ae_wrB_{}_wcB_{}_wrI_{}_wcI_{}_wC_{}_reductB_{}_rm_{}_{}'.format(w_rB,w_cB, w_rI, w_cI, w_mse, data_reduction_ratio, removal, str(kfold_number))\n",
    "\n",
    "device = torch.device(\"cuda:{}\".format(args.cuda))\n",
    "print(\"Loading data...\")\n",
    "\n",
    "train_loader, val_loader, train_dataset, val_dataset = get_trainValLoader(data_dir, k=kfold_number, spike_ready=False, batch_size=batch_size, shuffle=shuffle)\n",
    "test_loader, test_dataset = get_testLoader(data_dir, spike_ready=False, batch_size=batch_size, shuffle=shuffle)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/students/student6_16/anaconda3/lib/python3.7/site-packages/torch/nn/modules/rnn.py:51: UserWarning:\n",
      "\n",
      "dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.2 and num_layers=1\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "VRAE(n_epochs=1,batch_size=32,cuda=True)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_B = VRAEC(num_class=num_class,\n",
    "            sequence_length=sequence_length_B,\n",
    "            number_of_features = number_of_features_B,\n",
    "            hidden_size = hidden_size, \n",
    "            hidden_layer_depth = hidden_layer_depth,\n",
    "            latent_length = latent_length,\n",
    "            batch_size = batch_size,\n",
    "            learning_rate = learning_rate,\n",
    "            n_epochs = n_epochs,\n",
    "            dropout_rate = dropout_rate, \n",
    "            cuda = cuda,\n",
    "            print_every=print_every, \n",
    "            clip=clip, \n",
    "            max_grad_norm=max_grad_norm,\n",
    "            dload = logDir,\n",
    "            model_name=model_name_B,\n",
    "            header=header_B,\n",
    "            device = device)\n",
    "model_B.to(device)\n",
    "\n",
    "model_I = VRAEC(num_class=num_class,\n",
    "            sequence_length=sequence_length_I,\n",
    "            number_of_features = number_of_features_I,\n",
    "            hidden_size = hidden_size, \n",
    "            hidden_layer_depth = hidden_layer_depth,\n",
    "            latent_length = latent_length,\n",
    "            batch_size = batch_size,\n",
    "            learning_rate = learning_rate,\n",
    "            n_epochs = n_epochs,\n",
    "            dropout_rate = dropout_rate, \n",
    "            cuda = cuda,\n",
    "            print_every=print_every, \n",
    "            clip=clip, \n",
    "            max_grad_norm=max_grad_norm,\n",
    "            dload = logDir,\n",
    "            model_name=model_name_I,\n",
    "            header=header_I,\n",
    "            device = device)\n",
    "model_I.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = nn.CrossEntropyLoss()\n",
    "optimB = optim.Adam(model_B.parameters(), lr=learning_rate)\n",
    "optimI = optim.Adam(model_I.parameters(), lr=learning_rate)\n",
    "cl_loss_fn = nn.NLLLoss()\n",
    "recon_loss_fn = nn.MSELoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "last batch training: LB: 3.23, LI: 3.16, LC: 0.36 \n",
      " recon_B 384.38, cl_B 2.85, recon_I 241.81, cl_I 2.92\n",
      "Epoch 0: Loss: lc 0.319,  train_B 0.105, val_B 0.101, train_I0.100, val_I0.098, \n",
      "\t\t Acc: train_B 0.052, val_B 0.141, train_I 0.068, val_I 0.078\n",
      "--------------------\n",
      "Saving model at 0 epoch to models_and_stat/BT19_joint_ae_wrB_0.001_wcB_1_wrI_0.001_wcI_1_wC_1_reductB_1_rm_2_0.pt\n",
      "Saving model at 0 epoch to models_and_stat/IcubCNN_joint_ae_wrB_0.001_wcB_1_wrI_0.001_wcI_1_wC_1_reductB_1_rm_2_0.pt\n",
      "RAE training takes time 0:00:02.422372\n"
     ]
    }
   ],
   "source": [
    "# one stage training: with recon_loss and mse_loss\n",
    "training_start=datetime.now()\n",
    "\n",
    "epoch_train_loss_B = []\n",
    "epoch_train_acc_B = []\n",
    "epoch_val_loss_B = []\n",
    "epoch_val_acc_B = []\n",
    "max_val_acc_B = 0\n",
    "\n",
    "epoch_train_loss_I = []\n",
    "epoch_train_acc_I = []\n",
    "epoch_val_loss_I = []\n",
    "epoch_val_acc_I = []\n",
    "\n",
    "epoch_train_tot_loss = []\n",
    "epoch_val_tot_loss = []\n",
    "max_val_acc_I = 0\n",
    "\n",
    "for epoch in range(n_epochs):\n",
    "\n",
    "    # TRAIN\n",
    "    model_B.train()\n",
    "    model_I.train()\n",
    "\n",
    "    correct_B = 0\n",
    "    train_loss_B = 0\n",
    "    correct_I = 0\n",
    "    train_loss_I = 0\n",
    "    train_loss_tot = 0\n",
    "    train_num_B = 0\n",
    "    train_num_I = 0\n",
    "            \n",
    "    for i, (XI, XB,  y) in enumerate(train_loader):\n",
    "        if i >= len(train_loader)-removal:\n",
    "            break\n",
    "        XI, XB, y = XI.to(device), XB.to(device), y.long().to(device)\n",
    "        \n",
    "        if XI.size()[0] != batch_size:\n",
    "#             print(\"batch {} size {} < {}, skip\".format(i, x.size()[0], batch_size))\n",
    "            break\n",
    "    \n",
    "    \n",
    "        train_num_I += XI.size(0)\n",
    "        # train modelI\n",
    "        optimI.zero_grad()  \n",
    "        x_decoded_I, latent_I, output = model_I(XI)\n",
    "\n",
    "        # construct loss function\n",
    "        recon_loss_I = recon_loss_fn(x_decoded_I, XI)\n",
    "        cl_loss_I = cl_loss_fn(output, y)\n",
    "        loss_I = w_rI*recon_loss_I + w_cI*cl_loss_I\n",
    "\n",
    "        # compute classification acc\n",
    "        pred = output.data.max(1, keepdim=True)[1]  # get the index of the max log-probability\n",
    "        correct_I += pred.eq(y.data.view_as(pred)).long().cpu().sum().item()\n",
    "        # accumulator\n",
    "        train_loss_I += loss_I.item()\n",
    "\n",
    "        \n",
    "        # train partially on I \n",
    "        if i % data_reduction_ratio == 0:\n",
    "\n",
    "            train_num_B += XB.size(0)\n",
    "\n",
    "            # train model_B\n",
    "            optimB.zero_grad()  \n",
    "            x_decoded_B, latent_B, output = model_B(XB)\n",
    "\n",
    "            # construct loss function\n",
    "            recon_loss_B = recon_loss_fn(x_decoded_B, XB)\n",
    "            cl_loss_B = cl_loss_fn(output, y)\n",
    "            loss_B = w_rB*recon_loss_B + w_cB*cl_loss_B\n",
    "\n",
    "            # compute classification acc\n",
    "            pred = output.data.max(1, keepdim=True)[1]  # get the index of the max log-probability\n",
    "            correct_B += pred.eq(y.data.view_as(pred)).long().cpu().sum().item()\n",
    "            # accumulator\n",
    "            train_loss_B += loss_B.item()\n",
    "        \n",
    "            loss_C = F.mse_loss(latent_B, latent_I)\n",
    "            loss = loss_B + loss_I + w_mse*loss_C\n",
    "\n",
    "            if epoch < 20:\n",
    "                loss_B.backward()\n",
    "                loss_I.backward()\n",
    "            else:\n",
    "                loss.backward()\n",
    "\n",
    "            optimB.step() \n",
    "            optimI.step() \n",
    "        \n",
    "        else:\n",
    "            # only train model I\n",
    "            loss = loss_I\n",
    "            loss.backward()\n",
    "            optimI.step()\n",
    "            \n",
    "        train_loss_tot += loss.item()\n",
    "        \n",
    "        \n",
    "    if epoch < 20 or epoch%200 == 0:\n",
    "        print(\"last batch training: LB: {:.2f}, LI: {:.2f}, LC: {:.2f} \\n recon_B {:.2f}, cl_B {:.2f}, recon_I {:.2f}, cl_I {:.2f}\"\\\n",
    "              .format(loss_B, loss_I, loss_C, recon_loss_B, cl_loss_B, recon_loss_I, cl_loss_I))\n",
    "    \n",
    "    \n",
    "    # fill stats\n",
    "    train_accuracy_B = correct_B / train_num_B # len(train_loader.dataset)\n",
    "    train_loss_B /= train_num_B #len(train_loader.dataset)\n",
    "    epoch_train_loss_B.append(train_loss_B)\n",
    "    epoch_train_acc_B.append(train_accuracy_B) \n",
    "    \n",
    "    train_accuracy_I = correct_I / train_num_I # len(train_loader.dataset)\n",
    "    train_loss_I /= train_num_I #len(train_loader.dataset)\n",
    "    epoch_train_loss_I.append(train_loss_I)\n",
    "    epoch_train_acc_I.append(train_accuracy_I) \n",
    "    \n",
    "\n",
    "    # VALIDATION\n",
    "    model_B.eval()\n",
    "    model_I.eval()\n",
    "\n",
    "    correct_B = 0\n",
    "    val_loss_B = 0\n",
    "    correct_I = 0\n",
    "    val_loss_I = 0\n",
    "    val_loss_tot = 0\n",
    "    val_num = 0\n",
    "\n",
    "    for i, (XI, XB,  y) in enumerate(val_loader):\n",
    "        XI, XB, y = XI.to(device), XB.to(device), y.long().to(device)\n",
    "        \n",
    "        if XI.size()[0] != batch_size:\n",
    "#             print(\"batch {} size {} < {}, skip\".format(i, x.size()[0], batch_size))\n",
    "            break\n",
    "\n",
    "        val_num += XI.size(0)\n",
    "        \n",
    "        # eval model_B\n",
    "        x_decoded_B, latent_B, output = model_B(XB)\n",
    "        # construct loss function\n",
    "        recon_loss_B = recon_loss_fn(x_decoded_B, XB)\n",
    "        cl_loss_B = cl_loss_fn(output, y)\n",
    "        loss_B = w_rB*recon_loss_B + w_cB*cl_loss_B\n",
    "       \n",
    "        # compute classification acc\n",
    "        pred = output.data.max(1, keepdim=True)[1]  # get the index of the max log-probability\n",
    "        correct_B += pred.eq(y.data.view_as(pred)).long().cpu().sum().item()\n",
    "        # accumulator\n",
    "        val_loss_B += loss_B.item()\n",
    "        \n",
    "        \n",
    "        # eval modelI \n",
    "        x_decoded_I, latent_I, output = model_I(XI)\n",
    "        # construct loss function\n",
    "        recon_loss_I = recon_loss_fn(x_decoded_I, XI)\n",
    "        cl_loss_I = cl_loss_fn(output, y)\n",
    "        loss_I = w_rI*recon_loss_I + w_cI*cl_loss_I\n",
    "\n",
    "        # compute classification acc\n",
    "        pred = output.data.max(1, keepdim=True)[1]  # get the index of the max log-probability\n",
    "        correct_I += pred.eq(y.data.view_as(pred)).long().cpu().sum().item()\n",
    "        # accumulator\n",
    "        val_loss_I += loss_I.item()\n",
    "        \n",
    "        loss_C = F.mse_loss(latent_B, latent_I)\n",
    "        loss = loss_B + loss_I + w_mse*loss_C\n",
    "        val_loss_tot += loss.item()\n",
    "\n",
    "    # fill stats\n",
    "    val_accuracy_B = correct_B / val_num # len(train_loader.dataset)\n",
    "    val_loss_B /= val_num #len(train_loader.dataset)\n",
    "    epoch_val_loss_B.append(val_loss_B)\n",
    "    epoch_val_acc_B.append(val_accuracy_B) \n",
    "    \n",
    "    val_accuracy_I = correct_I / val_num # len(train_loader.dataset)\n",
    "    val_loss_I /= val_num #len(train_loader.dataset)\n",
    "    epoch_val_loss_I.append(val_loss_I)\n",
    "    epoch_val_acc_I.append(val_accuracy_I) \n",
    "    \n",
    "    if epoch < 20 or epoch%200 == 0:\n",
    "#         print(\"train_num {}, val_num {}\".format(train_num, val_num))\n",
    "        print(\"Epoch {}: Loss: lc {:.3f},  train_B {:.3f}, val_B {:.3f}, train_I{:.3f}, val_I{:.3f}, \\n\\t\\t Acc: train_B {:.3f}, val_B {:.3f}, train_I {:.3f}, val_I {:.3f}\"\\\n",
    "              .format(epoch, loss_C, train_loss_B, val_loss_B, train_loss_I, val_loss_I, train_accuracy_B, val_accuracy_B, train_accuracy_I, val_accuracy_I))\n",
    "        print(\"-\"*20)\n",
    "    # choose model\n",
    "    # TODO: not save at the same time, may have bad common representation\n",
    "    if max_val_acc_B <= val_accuracy_B:\n",
    "        model_dir = logDir + model_name_B + '.pt'\n",
    "        print(\"Saving model at {} epoch to {}\".format(epoch, model_dir))\n",
    "        max_val_acc_B = val_accuracy_B\n",
    "        torch.save(model_B.state_dict(), model_dir)\n",
    "\n",
    "    if max_val_acc_I <= val_accuracy_I:\n",
    "        model_dir = logDir + model_name_I + '.pt'\n",
    "        print(\"Saving model at {} epoch to {}\".format(epoch, model_dir))\n",
    "        max_val_acc_I = val_accuracy_I\n",
    "        torch.save(model_I.state_dict(), model_dir)\n",
    "    \n",
    "\n",
    "training_end =  datetime.now()\n",
    "training_time = training_end -training_start \n",
    "print(\"RAE training takes time {}\".format(training_time)) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "VRAE(n_epochs=1,batch_size=32,cuda=True)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_B.is_fitted = True\n",
    "model_I.is_fitted = True\n",
    "\n",
    "model_B.eval()\n",
    "model_I.eval()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "model_B_trained = VRAEC(num_class=num_class,\n",
    "            sequence_length=sequence_length_B,\n",
    "            number_of_features = number_of_features_B,\n",
    "            hidden_size = hidden_size, \n",
    "            hidden_layer_depth = hidden_layer_depth,\n",
    "            latent_length = latent_length,\n",
    "            batch_size = batch_size,\n",
    "            learning_rate = learning_rate,\n",
    "            n_epochs = n_epochs,\n",
    "            dropout_rate = dropout_rate, \n",
    "            cuda = cuda,\n",
    "            print_every=print_every, \n",
    "            clip=clip, \n",
    "            max_grad_norm=max_grad_norm,\n",
    "            dload = logDir,\n",
    "            model_name=model_name_B,\n",
    "            header=header_B,\n",
    "            w_r = w_r, \n",
    "            w_k = w_k, \n",
    "            w_c = w_c,\n",
    "            device = device)\n",
    "model_B_trained.load_state_dict(torch.load(logDir + model_name_B + '.pt'))\n",
    "model_B_trained.to(device)\n",
    "model_B_trained.eval()\n",
    "\n",
    "model_I_trained = VRAEC(num_class=num_class,\n",
    "            sequence_length=sequence_length_I,\n",
    "            number_of_features = number_of_features_I,\n",
    "            hidden_size = hidden_size, \n",
    "            hidden_layer_depth = hidden_layer_depth,\n",
    "            latent_length = latent_length,\n",
    "            batch_size = batch_size,\n",
    "            learning_rate = learning_rate,\n",
    "            n_epochs = n_epochs,\n",
    "            dropout_rate = dropout_rate, \n",
    "            cuda = cuda,\n",
    "            print_every=print_every, \n",
    "            clip=clip, \n",
    "            max_grad_norm=max_grad_norm,\n",
    "            dload = logDir,\n",
    "            model_name=model_name_I,\n",
    "            header=header_I,\n",
    "            w_r = w_r, \n",
    "            w_k = w_k, \n",
    "            w_c = w_c,\n",
    "            device = device)\n",
    "model_I_trained.load_state_dict(torch.load(logDir + model_name_I + '.pt'))\n",
    "model_I_trained.to(device)\n",
    "model_I_trained.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test accuracy for 0 fold 192 samples: B 0.13541666666666666, I 0.08854166666666667\n"
     ]
    }
   ],
   "source": [
    "# TEST\n",
    "correct_B = 0\n",
    "correct_I = 0\n",
    "test_num = 0\n",
    "\n",
    "for i, (XI, XB,  y) in enumerate(test_loader):\n",
    "    XI, XB, y = XI.to(device), XB.to(device), y.long().to(device)\n",
    "\n",
    "    if XI.size()[0] != batch_size:\n",
    "#             print(\"batch {} size {} < {}, skip\".format(i, x.size()[0], batch_size))\n",
    "        break\n",
    "\n",
    "    test_num += XI.size(0)\n",
    "\n",
    "    # test model_B\n",
    "    x_decoded_B, latent_B, output = model_B(XB)\n",
    "\n",
    "    # compute classification acc\n",
    "    pred = output.data.max(1, keepdim=True)[1]  # get the index of the max log-probability\n",
    "    correct_B += pred.eq(y.data.view_as(pred)).long().cpu().sum().item()\n",
    "\n",
    "    \n",
    "    # test modelI \n",
    "    x_decoded_I, latent_I, output = model_I(XI)\n",
    "\n",
    "    # compute classification acc\n",
    "    pred = output.data.max(1, keepdim=True)[1]  # get the index of the max log-probability\n",
    "    correct_I += pred.eq(y.data.view_as(pred)).long().cpu().sum().item()\n",
    "\n",
    "test_acc_B = correct_B/test_num\n",
    "test_acc_I = correct_I/test_num\n",
    "print('Test accuracy for {} fold {} samples: B {}, I {}'.format(str(kfold_number),test_num, test_acc_B, test_acc_I))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot_clustering(z_runs[0], y_val[0], engine='matplotlib', download = True, folder_name='figures', filefix='_BT19_joint_{}'.format(n_epochs))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot_clustering(z_runs[1], y_val[1], engine='matplotlib', download = True, folder_name='figures', filefix='_Icub_joint_{}'.format(n_epochs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dump results dict to BT19Icub_joint_ae_reductB_1_rm_2_wrB_0.001_wcB_1_wrI_0.001_wcI_1_wC_1_0.pkl\n"
     ]
    }
   ],
   "source": [
    "# save stats\n",
    "results_dict = {\"epoch_train_loss_B\": epoch_train_loss_B,\n",
    "                \"epoch_train_loss_I\": epoch_train_loss_I,\n",
    "                \"epoch_val_loss_B\": epoch_val_loss_B,\n",
    "                \"epoch_val_loss_I\": epoch_val_loss_I,\n",
    "                \"epoch_train_acc_B\": epoch_train_acc_B,\n",
    "                \"epoch_train_acc_I\": epoch_train_acc_I,\n",
    "                \"epoch_val_acc_B\": epoch_val_acc_B,\n",
    "                \"epoch_val_acc_I\": epoch_val_acc_I,\n",
    "                \"test_acc\": [test_acc_B, test_acc_I]}\n",
    "dict_name = \"BT19Icub_joint_ae_reductB_{}_rm_{}_wrB_{}_wcB_{}_wrI_{}_wcI_{}_wC_{}_{}.pkl\".format(data_reduction_ratio, removal, w_rB, w_cB, w_rI, w_cI, w_mse, str(kfold_number))\n",
    "pickle.dump(results_dict, open(logDir + dict_name, 'wb'))\n",
    "print(\"dump results dict to {}\".format(dict_name))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### plot the train acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 1080x504 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "assert n_epochs == len(epoch_train_acc_B), \"different epoch length {} {}\".format(n_epochs, len(epoch_train_acc_B))\n",
    "fig, ax = plt.subplots(figsize=(15, 7))\n",
    "ax.plot(np.arange(n_epochs), epoch_train_acc_B, label=\"train acc B\")\n",
    "ax.set_xlabel('epoch')\n",
    "ax.set_ylabel('acc')\n",
    "ax.grid(True)\n",
    "plt.legend(loc='upper right')\n",
    "figname = logDir + model_name_B+\"_train_acc.png\"\n",
    "plt.savefig(figname)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 1080x504 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "assert n_epochs == len(epoch_train_acc_I), \"different epoch length {} {}\".format(n_epochs, len(epoch_train_acc_I))\n",
    "fig, ax = plt.subplots(figsize=(15, 7))\n",
    "ax.plot(np.arange(n_epochs), epoch_train_acc_I, label=\"train acc I\")\n",
    "ax.set_xlabel('epoch')\n",
    "ax.set_ylabel('acc')\n",
    "ax.grid(True)\n",
    "plt.legend(loc='upper right')\n",
    "figname = logDir + model_name_I + \"_train_acc.png\"\n",
    "plt.savefig(figname)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
