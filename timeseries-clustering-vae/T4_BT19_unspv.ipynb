{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "python T4_BT19_unspv.py -k 0 -c 0\n",
    "'''\n",
    "\n",
    "# Import\n",
    "\n",
    "import os,sys\n",
    "CURRENT_TEST_DIR = os.getcwd()\n",
    "sys.path.append(CURRENT_TEST_DIR + \"/../new_iteration/\")\n",
    "import pickle\n",
    "import argparse\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from datetime import datetime\n",
    "\n",
    "from vrae.vrae import VRAEC\n",
    "from vrae.utils import *\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.utils import data as data2\n",
    "from torch.utils.data.dataset import random_split\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "from datetime import datetime\n",
    "from tas_utils_bs import get_trainValLoader, get_testLoader\n",
    "import plotly\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Parse argument\n",
    "# parser = argparse.ArgumentParser()\n",
    "# parser.add_argument(\"-k\", \"--kfold\", type=int, default=0, help=\"kfold_number for loading data\")\n",
    "# parser.add_argument(\"-r\", \"--reduction\", type=int, default=1, help=\"data reduction ratio for partial training\")\n",
    "# parser.add_argument(\"-c\", \"--cuda\", default=0, help=\"index of cuda gpu to use\")\n",
    "# args = parser.parse_args()\n",
    "\n",
    "# test_model=False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dummy class to replace argparser\n",
    "class Args:\n",
    "  kfold = 0\n",
    "  reduction = 1\n",
    "  cuda = '3'\n",
    "\n",
    "args=Args()\n",
    "test_model = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "load 0 kfold number, reduce data to 1 folds, put to cuda:3\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<torch._C.Generator at 0x7f8190552550>"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(\"load {} kfold number, reduce data to {} folds, put to cuda:{}\".format(args.kfold, args.reduction, args.cuda))\n",
    "\n",
    "# Set hyper params\n",
    "kfold_number = args.kfold\n",
    "data_reduction_ratio = args.reduction\n",
    "shuffle = False\n",
    "num_class = 20\n",
    "sequence_length = 400\n",
    "number_of_features = 19\n",
    "\n",
    "hidden_size = 90\n",
    "hidden_layer_depth = 1\n",
    "latent_length = 40\n",
    "batch_size = 32\n",
    "learning_rate = 0.0005\n",
    "n_epochs = 20\n",
    "\n",
    "learning_rate_2 = 0.01\n",
    "n_epochs_2 = 20\n",
    "\n",
    "dropout_rate = 0.2\n",
    "cuda = True # options: True, False\n",
    "print_every=30\n",
    "clip = True # options: True, False\n",
    "max_grad_norm=5\n",
    "header = None\n",
    "w_r = 0.01\n",
    "w_c = 1\n",
    "np.random.seed(1)\n",
    "torch.manual_seed(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading data...\n"
     ]
    }
   ],
   "source": [
    "# Load data\n",
    "data_dir = '../../new_data_folder/'\n",
    "kfold_number = 0\n",
    "\n",
    "logDir = 'models_and_stat/'\n",
    "model_name = 'BT19_unspv_wrI_{}_wC_{}_{}'.format(w_r, w_c, str(kfold_number))\n",
    "device = torch.device(\"cuda:{}\".format(args.cuda))\n",
    "print(\"Loading data...\")\n",
    "train_loader, val_loader, train_dataset, val_dataset = get_trainValLoader(data_dir, k=kfold_number, spike_ready=False, batch_size=batch_size, shuffle=shuffle)\n",
    "test_loader, test_dataset = get_testLoader(data_dir, spike_ready=False, batch_size=batch_size, shuffle=shuffle)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/students/student6_16/anaconda3/lib/python3.7/site-packages/torch/nn/modules/rnn.py:51: UserWarning:\n",
      "\n",
      "dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.2 and num_layers=1\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "VRAE(n_epochs=20,batch_size=32,cuda=True)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Create model\n",
    "model = VRAEC(num_class=num_class,\n",
    "            sequence_length=sequence_length,\n",
    "            number_of_features = number_of_features,\n",
    "            hidden_size = hidden_size, \n",
    "            hidden_layer_depth = hidden_layer_depth,\n",
    "            latent_length = latent_length,\n",
    "            batch_size = batch_size,\n",
    "            learning_rate = learning_rate,\n",
    "            n_epochs = n_epochs,\n",
    "            dropout_rate = dropout_rate,\n",
    "            cuda = cuda,\n",
    "            print_every=print_every, \n",
    "            clip=clip, \n",
    "            max_grad_norm=max_grad_norm,\n",
    "            dload = logDir,\n",
    "            model_name=model_name,\n",
    "            header=header,\n",
    "            device = device)\n",
    "\n",
    "model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
    "cl_loss_fn = nn.NLLLoss()\n",
    "recon_loss_fn = nn.MSELoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1st stage training: with recon_loss\n",
    "training_start=datetime.now()\n",
    "#split fit\n",
    "epoch_train_loss = []\n",
    "epoch_train_acc = []\n",
    "epoch_val_loss = []\n",
    "epoch_val_acc = []\n",
    "max_val_acc = 0\n",
    "\n",
    "for epoch in range(n_epochs):\n",
    "    \n",
    "    # TRAIN\n",
    "    model.train()\n",
    "    correct = 0\n",
    "    train_loss = 0\n",
    "    train_num = 0\n",
    "    for i, (XI, XB,  y) in enumerate(train_loader):\n",
    "        if model.header == 'CNN':\n",
    "            x = XI\n",
    "        else:\n",
    "            x = XB\n",
    "        x, y = x.to(device), y.long().to(device)\n",
    "        if x.size()[0] != batch_size:\n",
    "#             print(\"batch {} size {} < {}, skip\".format(i, x.size()[0], batch_size))\n",
    "            break\n",
    "        train_num += x.size(0)\n",
    "        optimizer.zero_grad()\n",
    "  \n",
    "        x_decoded, latent, output = model(x)\n",
    "\n",
    "        # construct loss function\n",
    "        loss = recon_loss_fn(x_decoded, x)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        # compute classification acc\n",
    "        pred = output.data.max(1, keepdim=True)[1]  # get the index of the max log-probability\n",
    "        correct += pred.eq(y.data.view_as(pred)).long().cpu().sum().item()\n",
    "\n",
    "        # accumulator\n",
    "        train_loss += loss.item()\n",
    "\n",
    "    # fill stats\n",
    "    if epoch < 20 or epoch%200 == 0:\n",
    "        print(\"train last batch: recon_loss {}\".format(loss))\n",
    "    train_accuracy = correct / train_num \n",
    "    train_loss /= train_num\n",
    "    epoch_train_loss.append(train_loss)\n",
    "    epoch_train_acc.append(train_accuracy) \n",
    "    \n",
    "    # VALIDATION\n",
    "    model.eval()\n",
    "    correct = 0\n",
    "    val_loss = 0\n",
    "    val_num = 0\n",
    "    for i, (XI, XB,  y) in enumerate(val_loader):\n",
    "        if model.header == 'CNN':\n",
    "            x = XI\n",
    "        else:\n",
    "            x = XB\n",
    "        x, y = x.to(device), y.long().to(device)\n",
    "        if x.size()[0] != batch_size:\n",
    "#             print(\"batch {} size {} < {}, skip\".format(i, x.size()[0], batch_size))\n",
    "            break\n",
    "        val_num += x.size(0)\n",
    "        x_decoded, latent, output = model(x)\n",
    "\n",
    "        # construct loss function\n",
    "        loss = recon_loss_fn(x_decoded, x)\n",
    "        \n",
    "        # compute classification acc\n",
    "        pred = output.data.max(1, keepdim=True)[1]  # get the index of the max log-probability\n",
    "        correct += pred.eq(y.data.view_as(pred)).long().cpu().sum().item()\n",
    "        \n",
    "        # accumulator\n",
    "        val_loss += loss.item()\n",
    "    \n",
    "#     print(\"test last batch: recon_loss {}, kl_loss {}, cl_loss {}\".format(recon_loss, kl_loss, cl_loss))\n",
    "    # fill stats\n",
    "    val_accuracy = correct / val_num# / len(val_loader.dataset)\n",
    "    val_loss /= val_num #len(val_loader.dataset)\n",
    "\n",
    "    epoch_val_loss.append(val_loss)  # only save the last batch\n",
    "    epoch_val_acc.append(val_accuracy)\n",
    "    \n",
    "    if epoch < 20 or epoch%200 == 0:\n",
    "        print('Epoch: {} Loss: train {}, valid {}. Accuracy: train: {}, valid {}'.format(epoch, train_loss, val_loss, train_accuracy, val_accuracy))\n",
    "    \n",
    "    # choose model\n",
    "    if max_val_acc <= val_accuracy:\n",
    "        model_dir = logDir + model_name + '.pt'\n",
    "        print('Saving model at {} epoch to{}'.format(epoch, model_dir))\n",
    "        max_val_acc = val_accuracy\n",
    "        torch.save(model.state_dict(), model_dir)\n",
    "        \n",
    "training_end =  datetime.now()\n",
    "training_time = training_end -training_start \n",
    "print(\"training takes time {}\".format(training_time))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2nd stage training: train classifier using classification loss\n",
    "epoch_cl_train_loss = []\n",
    "epoch_cl_train_acc = []\n",
    "epoch_cl_val_loss = []\n",
    "epoch_cl_val_acc = []\n",
    "max_val_acc = 0\n",
    "\n",
    "optimizer = optim.Adam(model.parameters(), lr=learning_rate_2)\n",
    "\n",
    "for epoch in range(n_epochs_2):\n",
    "    \n",
    "    # TRAIN\n",
    "    model.train()\n",
    "    correct = 0\n",
    "    train2_loss = 0\n",
    "    train_num = 0\n",
    "    \n",
    "    # freeze params except for the classifier\n",
    "    trained_names = ['classifier.0.bias', 'classifier.0.weight']\n",
    "    for name, param in model.named_parameters():\n",
    "        if name in trained_names:\n",
    "            param.requires_grad = True\n",
    "        else:\n",
    "            param.requires_grad = False\n",
    "    \n",
    "    for i, (XI, XB,  y) in enumerate(train_loader):\n",
    "        if model.header == 'CNN':\n",
    "            x = XI\n",
    "        else:\n",
    "            x = XB\n",
    "        x, y = x.to(device), y.long().to(device)\n",
    "        if x.size()[0] != batch_size:\n",
    "#             print(\"batch {} size {} < {}, skip\".format(i, x.size()[0], batch_size))\n",
    "            break\n",
    "        train_num += x.size(0)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        x_decoded, latent, output = model(x)\n",
    "\n",
    "        # construct loss function\n",
    "        loss = cl_loss_fn(output, y)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        # compute classification acc\n",
    "        pred = output.data.max(1, keepdim=True)[1]  # get the index of the max log-probability\n",
    "        correct += pred.eq(y.data.view_as(pred)).long().cpu().sum().item()\n",
    "\n",
    "        # accumulator\n",
    "        train2_loss += loss.item()\n",
    "\n",
    "    # fill stats\n",
    "    if epoch < 20 or epoch%200 == 0:\n",
    "        print(\"train last batch:  cl_loss {}\".format(loss))\n",
    "    train2_accuracy = correct / train_num # len(train_loader.dataset)\n",
    "    train2_loss /= train_num #len(train_loader.dataset)\n",
    "    epoch_cl_train_loss.append(train2_loss)\n",
    "    epoch_cl_train_acc.append(train2_accuracy) \n",
    "    \n",
    "    # VALIDATION\n",
    "    model.eval()\n",
    "    correct = 0\n",
    "    val2_loss = 0\n",
    "    val_num = 0\n",
    "    for i, (XI, XB,  y) in enumerate(val_loader):\n",
    "        if model.header == 'CNN':\n",
    "            x = XI\n",
    "        else:\n",
    "            x = XB\n",
    "        x, y = x.to(device), y.long().to(device)\n",
    "        if x.size()[0] != batch_size:\n",
    "#             print(\"batch {} size {} < {}, skip\".format(i, x.size()[0], batch_size))\n",
    "            break\n",
    "        val_num += x.size(0)\n",
    "        x_decoded, latent, output = model(x)\n",
    "\n",
    "        # construct loss function\n",
    "        loss = cl_loss_fn(output, y)\n",
    "    \n",
    "        # compute classification acc\n",
    "        pred = output.data.max(1, keepdim=True)[1]  # get the index of the max log-probability\n",
    "        correct += pred.eq(y.data.view_as(pred)).long().cpu().sum().item()\n",
    "        \n",
    "        # accumulator\n",
    "        val2_loss += loss.item()\n",
    "\n",
    "    # fill stats\n",
    "    val2_accuracy = correct / val_num# / len(val_loader.dataset)\n",
    "    val2_loss /= val_num #len(val_loader.dataset)\n",
    "\n",
    "    epoch_cl_val_loss.append(val2_loss)  # only save the last batch\n",
    "    epoch_cl_val_acc.append(val2_accuracy)\n",
    "    if epoch < 20 or epoch%200 == 0:\n",
    "        print('Epoch: {} Loss: train {}, valid {}. Accuracy: train: {}, valid {}'.format(epoch, train_loss, val_loss, train_accuracy, val_accuracy))\n",
    "    \n",
    "    # choose model\n",
    "    if max_val_acc <= val_accuracy:\n",
    "        model_dir = logDir + model_name + '.pt'\n",
    "        print('Saving model at {} epoch to{}'.format(epoch, model_dir))\n",
    "        max_val_acc = val_accuracy\n",
    "        torch.save(model.state_dict(), model_dir)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "if test_model:\n",
    "    \n",
    "    pretrained_model_path = \"BT19_unspv_wrI_0.01_wC_1_0.pt\"\n",
    "    model.load_state_dict(torch.load(logDir+pretrained_model_path))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "VRAE(n_epochs=20,batch_size=32,cuda=True)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.is_fitted = True\n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# testing set check\n",
    "net_trained = VRAEC(num_class=num_class,\n",
    "            sequence_length=sequence_length,\n",
    "            number_of_features = number_of_features,\n",
    "            hidden_size = hidden_size, \n",
    "            hidden_layer_depth = hidden_layer_depth,\n",
    "            latent_length = latent_length,\n",
    "            batch_size = batch_size,\n",
    "            learning_rate = learning_rate,\n",
    "            n_epochs = n_epochs,\n",
    "            dropout_rate = dropout_rate,\n",
    "            cuda = cuda,\n",
    "            print_every=print_every, \n",
    "            clip=clip, \n",
    "            max_grad_norm=max_grad_norm,\n",
    "            dload = logDir,\n",
    "            model_name=model_name,\n",
    "            header=header,\n",
    "            device = device)\n",
    "\n",
    "net_trained.load_state_dict(torch.load(logDir + model_name + '.pt'))\n",
    "net_trained.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " test batch 6 size 8 < 32, skip\n",
      "Test accuracy for 0  fold :  0.8229166666666666\n"
     ]
    }
   ],
   "source": [
    "correct = 0\n",
    "test_num = 0\n",
    "for i, (XI, XB,  y) in enumerate(test_loader):\n",
    "    if model.header == 'CNN':\n",
    "        x = XI\n",
    "    else:\n",
    "        x = XB\n",
    "    x, y = x.to(device), y.long().to(device)\n",
    "    \n",
    "    if x.size(0) != batch_size:\n",
    "        print(\" test batch {} size {} < {}, skip\".format(i, x.size()[0], batch_size))\n",
    "        break\n",
    "    test_num += x.size(0)\n",
    "    x_decoded, latent, output = model(x)\n",
    "\n",
    "    # compute classification acc\n",
    "    pred = output.data.max(1, keepdim=True)[1]  # get the index of the max log-probability\n",
    "    correct += pred.eq(y.data.view_as(pred)).long().cpu().sum().item()\n",
    "    \n",
    "test_acc = correct / test_num #len(test_loader.dataset)\n",
    "\n",
    "print('Test accuracy for', str(kfold_number), ' fold : ', test_acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save stats\n",
    "all_stats = {\"epoch_train_loss\": epoch_train_loss,\n",
    "             \"epoch_train_acc\": epoch_train_acc,\n",
    "             \"epoch_val_loss\": epoch_val_loss,\n",
    "             \"epoch_val_acc\": epoch_val_acc,\n",
    "             \"epoch_cl_train_loss\":epoch_cl_train_loss,\n",
    "             \"epoch_cl_train_acc\": epoch_cl_train_acc,\n",
    "             \"epoch_cl_val_loss\": epoch_cl_val_loss,\n",
    "             \"epoch_cl_val_acc\": epoch_cl_val_acc,\n",
    "             \"test_acc\": test_acc}\n",
    "\n",
    "dict_name = logDir + model_name + '_stats.pkl'\n",
    "pickle.dump(all_stats, open(dict_name, 'wb'))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert n_epochs == len(epoch_train_acc), \"different epoch length {} {}\".format(n_epochs, len(epoch_train_acc))\n",
    "fig, ax = plt.subplots(figsize=(15, 7))\n",
    "ax.plot(np.arange(n_epochs), epoch_train_acc, label=\"train acc\")\n",
    "ax.set_xlabel('epoch')\n",
    "ax.set_ylabel('acc')\n",
    "ax.grid(True)\n",
    "plt.legend(loc='upper right')\n",
    "figname = logDir + model_name +\"_train1_acc.png\"\n",
    "plt.savefig(figname)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(epoch_cl_train_acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert n_epochs_2 == len(epoch_cl_train_acc), \"different epoch length {} {}\".format(n_epochs_2, len(epoch_cl_train_acc))\n",
    "fig, ax = plt.subplots(figsize=(15, 7))\n",
    "ax.plot(np.arange(n_epochs_2), epoch_cl_train_acc, label=\"train acc\")\n",
    "ax.set_xlabel('epoch')\n",
    "ax.set_ylabel('acc')\n",
    "ax.grid(True)\n",
    "plt.legend(loc='upper right')\n",
    "figname = logDir + model_name +\"_train2_acc.png\"\n",
    "plt.savefig(figname)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
