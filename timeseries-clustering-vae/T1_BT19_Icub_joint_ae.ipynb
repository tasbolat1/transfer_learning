{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "python T1_BT19_Icub_joint_ae.py -k 0 -c 0 -r 1\n",
    "\n",
    "Note: still use one-stage training with both recon loss and classification loss\n",
    "'''\n",
    "# Import\n",
    "\n",
    "import os,sys\n",
    "CURRENT_TEST_DIR = os.getcwd()\n",
    "sys.path.append(CURRENT_TEST_DIR + \"/../new_iteration/\")\n",
    "import pickle\n",
    "import argparse\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from datetime import datetime\n",
    "\n",
    "from vrae.vrae import VRAEC\n",
    "from vrae.utils import *\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.utils import data as data2\n",
    "from torch.utils.data.dataset import random_split\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "from datetime import datetime\n",
    "from tas_utils_bs import get_trainValLoader, get_testLoader\n",
    "import plotly\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Parse argument\n",
    "# parser = argparse.ArgumentParser()\n",
    "# parser.add_argument(\"-k\", \"--kfold\", type=int, default=0, help=\"kfold_number for loading data\")\n",
    "# parser.add_argument(\"-r\", \"--reduction\", type=int, default=1, help=\"data reduction ratio for partial training\")\n",
    "# parser.add_argument(\"-c\", \"--cuda\", default=0, help=\"index of cuda gpu to use\")\n",
    "# args = parser.parse_args()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dummy class to replace argparser\n",
    "class Args:\n",
    "  kfold = 0\n",
    "  reduction = 1\n",
    "  cuda = '0'\n",
    "\n",
    "args=Args()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"load {} kfold number, reduce data to {} folds, put to cuda:{}\".format(args.kfold, args.reduction, args.cuda))\n",
    "\n",
    "# Set hyper params\n",
    "kfold_number = args.kfold\n",
    "data_reduction_ratio = args.reduction\n",
    "shuffle = False\n",
    "num_class = 20\n",
    "sequence_length_B = 400\n",
    "sequence_length_I = 75\n",
    "number_of_features_B = 19\n",
    "number_of_features_I = 60\n",
    "\n",
    "hidden_size = 90\n",
    "hidden_layer_depth = 1\n",
    "latent_length = 40\n",
    "batch_size = 32\n",
    "learning_rate = 0.0005\n",
    "n_epochs = 20\n",
    "\n",
    "learning_rate_2 = 0.01\n",
    "n_epochs_2 = 20\n",
    "\n",
    "dropout_rate = 0.2\n",
    "cuda = True # options: True, False\n",
    "print_every=30\n",
    "clip = True # options: True, False\n",
    "max_grad_norm=5\n",
    "header_B = None\n",
    "header_I = \"CNN\"\n",
    "\n",
    "w_mse = 1 # mse between latent vectors\n",
    "w_rB = 0.01 # recon for B\n",
    "w_rI = 0.01 # recon for I\n",
    "w_cB = 1 # classify for B\n",
    "w_cI = 1 # classify for I\n",
    "\n",
    "\n",
    "np.random.seed(1)\n",
    "torch.manual_seed(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load data and preprocess"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load data\n",
    "data_dir = '../../new_data_folder/'\n",
    "\n",
    "logDir = 'models_and_stat/'\n",
    "# new model\n",
    "model_name_B = 'BT19_joint_ae_wrB_{}_wcB_{}_wrI_{}_wcI_{}_wC_{}_{}'.format(w_rB,w_cB, w_rI, w_cI, w_mse, str(kfold_number))\n",
    "model_name_I = 'IcubCNN_joint_ae_wrB_{}_wcB_{}_wrI_{}_wcI_{}_wC_{}_{}'.format(w_rB,w_cB, w_rI, w_cI, w_mse, str(kfold_number))\n",
    "\n",
    "device = torch.device(\"cuda:{}\".format(args.cuda))\n",
    "print(\"Loading data...\")\n",
    "\n",
    "train_loader, val_loader, train_dataset, val_dataset = get_trainValLoader(data_dir, k=kfold_number, spike_ready=False, batch_size=batch_size, shuffle=shuffle)\n",
    "test_loader, test_dataset = get_testLoader(data_dir, spike_ready=False, batch_size=batch_size, shuffle=shuffle)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_B = VRAEC(num_class=num_class,\n",
    "            sequence_length=sequence_length_B,\n",
    "            number_of_features = number_of_features_B,\n",
    "            hidden_size = hidden_size, \n",
    "            hidden_layer_depth = hidden_layer_depth,\n",
    "            latent_length = latent_length,\n",
    "            batch_size = batch_size,\n",
    "            learning_rate = learning_rate,\n",
    "            n_epochs = n_epochs,\n",
    "            dropout_rate = dropout_rate, \n",
    "            cuda = cuda,\n",
    "            print_every=print_every, \n",
    "            clip=clip, \n",
    "            max_grad_norm=max_grad_norm,\n",
    "            dload = logDir,\n",
    "            model_name=model_name_B,\n",
    "            header=header_B,\n",
    "            device = device)\n",
    "model_B.to(device)\n",
    "\n",
    "model_I = VRAEC(num_class=num_class,\n",
    "            sequence_length=sequence_length_I,\n",
    "            number_of_features = number_of_features_I,\n",
    "            hidden_size = hidden_size, \n",
    "            hidden_layer_depth = hidden_layer_depth,\n",
    "            latent_length = latent_length,\n",
    "            batch_size = batch_size,\n",
    "            learning_rate = learning_rate,\n",
    "            n_epochs = n_epochs,\n",
    "            dropout_rate = dropout_rate, \n",
    "            cuda = cuda,\n",
    "            print_every=print_every, \n",
    "            clip=clip, \n",
    "            max_grad_norm=max_grad_norm,\n",
    "            dload = logDir,\n",
    "            model_name=model_name_I,\n",
    "            header=header_I,\n",
    "            device = device)\n",
    "model_I.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = nn.CrossEntropyLoss()\n",
    "optimB = optim.Adam(model_B.parameters(), lr=learning_rate)\n",
    "optimI = optim.Adam(model_I.parameters(), lr=learning_rate)\n",
    "cl_loss_fn = nn.NLLLoss()\n",
    "recon_loss_fn = nn.MSELoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# one stage training: with recon_loss and mse_loss\n",
    "training_start=datetime.now()\n",
    "\n",
    "epoch_train_loss_B = []\n",
    "epoch_train_acc_B = []\n",
    "epoch_val_loss_B = []\n",
    "epoch_val_acc_B = []\n",
    "max_val_acc_B = 0\n",
    "\n",
    "epoch_train_loss_I = []\n",
    "epoch_train_acc_I = []\n",
    "epoch_val_loss_I = []\n",
    "epoch_val_acc_I = []\n",
    "\n",
    "epoch_train_tot_loss = []\n",
    "epoch_val_tot_loss = []\n",
    "max_val_acc_I = 0\n",
    "\n",
    "for epoch in range(n_epochs):\n",
    "\n",
    "    # TRAIN\n",
    "    model_B.train()\n",
    "    model_I.train()\n",
    "\n",
    "    correct_B = 0\n",
    "    train_loss_B = 0\n",
    "    correct_I = 0\n",
    "    train_loss_I = 0\n",
    "    train_loss_tot = 0\n",
    "    train_num = 0\n",
    "            \n",
    "    for i, (XI, XB,  y) in enumerate(train_loader):\n",
    "        XI, XB, y = XI.to(device), XB.to(device), y.long().to(device)\n",
    "        \n",
    "        if XI.size()[0] != batch_size:\n",
    "#             print(\"batch {} size {} < {}, skip\".format(i, x.size()[0], batch_size))\n",
    "            break\n",
    "\n",
    "        train_num += XI.size(0)\n",
    "        \n",
    "        # train model_B\n",
    "        optimB.zero_grad()  \n",
    "        x_decoded_B, latent_B, output = model_B(XB)\n",
    "        \n",
    "        # construct loss function\n",
    "        recon_loss_B = recon_loss_fn(x_decoded_B, XB)\n",
    "        cl_loss_B = cl_loss_fn(output, y)\n",
    "        loss_B = w_rB*recon_loss_B + w_cB*cl_loss_B\n",
    "        \n",
    "        # compute classification acc\n",
    "        pred = output.data.max(1, keepdim=True)[1]  # get the index of the max log-probability\n",
    "        correct_B += pred.eq(y.data.view_as(pred)).long().cpu().sum().item()\n",
    "        # accumulator\n",
    "        train_loss_B += loss_B.item()\n",
    "        \n",
    "        \n",
    "        # train modelI\n",
    "        optimI.zero_grad()  \n",
    "        x_decoded_I, latent_I, output = model_I(XI)\n",
    "        \n",
    "        # construct loss function\n",
    "        recon_loss_I = recon_loss_fn(x_decoded_I, XI)\n",
    "        cl_loss_I = cl_loss_fn(output, y)\n",
    "        loss_I = w_rI*recon_loss_I + w_cI*cl_loss_I\n",
    "\n",
    "        # compute classification acc\n",
    "        pred = output.data.max(1, keepdim=True)[1]  # get the index of the max log-probability\n",
    "        correct_I += pred.eq(y.data.view_as(pred)).long().cpu().sum().item()\n",
    "        # accumulator\n",
    "        train_loss_I += loss_I.item()\n",
    "        \n",
    "        loss_C = F.mse_loss(latent_B, latent_I)\n",
    "        loss = loss_B + loss_I + w_mse*loss_C\n",
    "        \n",
    "        if epoch < 20:\n",
    "            loss_B.backward()\n",
    "            loss_I.backward()\n",
    "        else:\n",
    "            loss.backward()\n",
    "        \n",
    "        train_loss_tot += loss.item()\n",
    "        \n",
    "        optimB.step() \n",
    "        optimI.step() \n",
    "    \n",
    "    if epoch < 20 or epoch%200 == 0:\n",
    "        print(\"last batch training: LB: {:.2f}, LI: {:.2f}, LC: {:.2f} \\n recon_B {:.2f}, cl_B {:.2f}, recon_I {:.2f}, cl_I {:.2f}\"\\\n",
    "              .format(loss_B, loss_I, loss_C, recon_loss_B, cl_loss_B, recon_loss_I, cl_loss_I))\n",
    "    \n",
    "    \n",
    "    # fill stats\n",
    "    train_accuracy_B = correct_B / train_num # len(train_loader.dataset)\n",
    "    train_loss_B /= train_num #len(train_loader.dataset)\n",
    "    epoch_train_loss_B.append(train_loss_B)\n",
    "    epoch_train_acc_B.append(train_accuracy_B) \n",
    "    \n",
    "    train_accuracy_I = correct_I / train_num # len(train_loader.dataset)\n",
    "    train_loss_I /= train_num #len(train_loader.dataset)\n",
    "    epoch_train_loss_I.append(train_loss_I)\n",
    "    epoch_train_acc_I.append(train_accuracy_I) \n",
    "    \n",
    "\n",
    "    # VALIDATION\n",
    "    model_B.eval()\n",
    "    model_I.eval()\n",
    "\n",
    "    correct_B = 0\n",
    "    val_loss_B = 0\n",
    "    correct_I = 0\n",
    "    val_loss_I = 0\n",
    "    val_loss_tot = 0\n",
    "    val_num = 0\n",
    "\n",
    "    for i, (XI, XB,  y) in enumerate(val_loader):\n",
    "        XI, XB, y = XI.to(device), XB.to(device), y.long().to(device)\n",
    "        \n",
    "        if XI.size()[0] != batch_size:\n",
    "#             print(\"batch {} size {} < {}, skip\".format(i, x.size()[0], batch_size))\n",
    "            break\n",
    "\n",
    "        val_num += XI.size(0)\n",
    "        \n",
    "        # eval model_B\n",
    "        x_decoded_B, latent_B, output = model_B(XB)\n",
    "        # construct loss function\n",
    "        recon_loss_B = recon_loss_fn(x_decoded_B, XB)\n",
    "        cl_loss_B = cl_loss_fn(output, y)\n",
    "        loss_B = w_rB*recon_loss_B + w_cB*cl_loss_B\n",
    "       \n",
    "        # compute classification acc\n",
    "        pred = output.data.max(1, keepdim=True)[1]  # get the index of the max log-probability\n",
    "        correct_B += pred.eq(y.data.view_as(pred)).long().cpu().sum().item()\n",
    "        # accumulator\n",
    "        val_loss_B += loss_B.item()\n",
    "        \n",
    "        \n",
    "        # eval modelI \n",
    "        x_decoded_I, latent_I, output = model_I(XI)\n",
    "        # construct loss function\n",
    "        recon_loss_I = recon_loss_fn(x_decoded_I, XI)\n",
    "        cl_loss_I = cl_loss_fn(output, y)\n",
    "        loss_I = w_rI*recon_loss_I + w_cI*cl_loss_I\n",
    "\n",
    "        # compute classification acc\n",
    "        pred = output.data.max(1, keepdim=True)[1]  # get the index of the max log-probability\n",
    "        correct_I += pred.eq(y.data.view_as(pred)).long().cpu().sum().item()\n",
    "        # accumulator\n",
    "        val_loss_I += loss_I.item()\n",
    "        \n",
    "        loss_C = F.mse_loss(latent_B, latent_I)\n",
    "        loss = loss_B + loss_I + w_mse*loss_C\n",
    "        val_loss_tot += loss.item()\n",
    "\n",
    "    # fill stats\n",
    "    val_accuracy_B = correct_B / val_num # len(train_loader.dataset)\n",
    "    val_loss_B /= val_num #len(train_loader.dataset)\n",
    "    epoch_val_loss_B.append(val_loss_B)\n",
    "    epoch_val_acc_B.append(val_accuracy_B) \n",
    "    \n",
    "    val_accuracy_I = correct_I / val_num # len(train_loader.dataset)\n",
    "    val_loss_I /= val_num #len(train_loader.dataset)\n",
    "    epoch_val_loss_I.append(val_loss_I)\n",
    "    epoch_val_acc_I.append(val_accuracy_I) \n",
    "    \n",
    "    if epoch < 20 or epoch%200 == 0:\n",
    "#         print(\"train_num {}, val_num {}\".format(train_num, val_num))\n",
    "        print(\"Epoch {}: Loss: lc {:.3f},  train_B {:.3f}, val_B {:.3f}, train_I{:.3f}, val_I{:.3f}, \\n\\t\\t Acc: train_B {:.3f}, val_B {:.3f}, train_I {:.3f}, val_I {:.3f}\"\\\n",
    "              .format(epoch, loss_C, train_loss_B, val_loss_B, train_loss_I, val_loss_I, train_accuracy_B, val_accuracy_B, train_accuracy_I, val_accuracy_I))\n",
    "        print(\"-\"*20)\n",
    "    # choose model\n",
    "    # TODO: not save at the same time, may have bad common representation\n",
    "    if max_val_acc_B <= val_accuracy_B:\n",
    "        model_dir = logDir + model_name_B + '.pt'\n",
    "        print(\"Saving model at {} epoch to {}\".format(epoch, model_dir))\n",
    "        max_val_acc_B = val_accuracy_B\n",
    "        torch.save(model_B.state_dict(), model_dir)\n",
    "\n",
    "    if max_val_acc_I <= val_accuracy_I:\n",
    "        model_dir = logDir + model_name_I + '.pt'\n",
    "        print(\"Saving model at {} epoch to {}\".format(epoch, model_dir))\n",
    "        max_val_acc_I = val_accuracy_I\n",
    "        torch.save(model_I.state_dict(), model_dir)\n",
    "    \n",
    "\n",
    "training_end =  datetime.now()\n",
    "training_time = training_end -training_start \n",
    "print(\"RAE training takes time {}\".format(training_time)) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_B.is_fitted = True\n",
    "model_I.is_fitted = True\n",
    "\n",
    "model_B.eval()\n",
    "model_I.eval()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "model_B_trained = VRAEC(num_class=num_class,\n",
    "            sequence_length=sequence_length_B,\n",
    "            number_of_features = number_of_features_B,\n",
    "            hidden_size = hidden_size, \n",
    "            hidden_layer_depth = hidden_layer_depth,\n",
    "            latent_length = latent_length,\n",
    "            batch_size = batch_size,\n",
    "            learning_rate = learning_rate,\n",
    "            n_epochs = n_epochs,\n",
    "            dropout_rate = dropout_rate, \n",
    "            cuda = cuda,\n",
    "            print_every=print_every, \n",
    "            clip=clip, \n",
    "            max_grad_norm=max_grad_norm,\n",
    "            dload = logDir,\n",
    "            model_name=model_name_B,\n",
    "            header=header_B,\n",
    "            w_r = w_r, \n",
    "            w_k = w_k, \n",
    "            w_c = w_c,\n",
    "            device = device)\n",
    "model_B_trained.load_state_dict(torch.load(logDir + model_name_B + '.pt'))\n",
    "model_B_trained.to(device)\n",
    "model_B_trained.eval()\n",
    "\n",
    "model_I_trained = VRAEC(num_class=num_class,\n",
    "            sequence_length=sequence_length_I,\n",
    "            number_of_features = number_of_features_I,\n",
    "            hidden_size = hidden_size, \n",
    "            hidden_layer_depth = hidden_layer_depth,\n",
    "            latent_length = latent_length,\n",
    "            batch_size = batch_size,\n",
    "            learning_rate = learning_rate,\n",
    "            n_epochs = n_epochs,\n",
    "            dropout_rate = dropout_rate, \n",
    "            cuda = cuda,\n",
    "            print_every=print_every, \n",
    "            clip=clip, \n",
    "            max_grad_norm=max_grad_norm,\n",
    "            dload = logDir,\n",
    "            model_name=model_name_I,\n",
    "            header=header_I,\n",
    "            w_r = w_r, \n",
    "            w_k = w_k, \n",
    "            w_c = w_c,\n",
    "            device = device)\n",
    "model_I_trained.load_state_dict(torch.load(logDir + model_name_I + '.pt'))\n",
    "model_I_trained.to(device)\n",
    "model_I_trained.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TEST\n",
    "correct_B = 0\n",
    "correct_I = 0\n",
    "test_num = 0\n",
    "\n",
    "for i, (XI, XB,  y) in enumerate(test_loader):\n",
    "    XI, XB, y = XI.to(device), XB.to(device), y.long().to(device)\n",
    "\n",
    "    if XI.size()[0] != batch_size:\n",
    "#             print(\"batch {} size {} < {}, skip\".format(i, x.size()[0], batch_size))\n",
    "        break\n",
    "\n",
    "    test_num += XI.size(0)\n",
    "\n",
    "    # test model_B\n",
    "    x_decoded_B, latent_B, output = model_B(XB)\n",
    "\n",
    "    # compute classification acc\n",
    "    pred = output.data.max(1, keepdim=True)[1]  # get the index of the max log-probability\n",
    "    correct_B += pred.eq(y.data.view_as(pred)).long().cpu().sum().item()\n",
    "\n",
    "    \n",
    "    # test modelI \n",
    "    x_decoded_I, latent_I, output = model_I(XI)\n",
    "\n",
    "    # compute classification acc\n",
    "    pred = output.data.max(1, keepdim=True)[1]  # get the index of the max log-probability\n",
    "    correct_I += pred.eq(y.data.view_as(pred)).long().cpu().sum().item()\n",
    "\n",
    "test_acc_B = correct_B/test_num\n",
    "test_acc_I = correct_I/test_num\n",
    "print('Test accuracy for {} fold {} samples: B {}, I {}'.format(str(kfold_number),test_num, test_acc_B, test_acc_I))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot_clustering(z_runs[0], y_val[0], engine='matplotlib', download = True, folder_name='figures', filefix='_BT19_joint_{}'.format(n_epochs))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot_clustering(z_runs[1], y_val[1], engine='matplotlib', download = True, folder_name='figures', filefix='_Icub_joint_{}'.format(n_epochs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save stats\n",
    "results_dict = {\"epoch_train_loss_B\": epoch_train_loss_B,\n",
    "                \"epoch_train_loss_I\": epoch_train_loss_I,\n",
    "                \"epoch_val_loss_B\": epoch_val_loss_B,\n",
    "                \"epoch_val_loss_I\": epoch_val_loss_I,\n",
    "                \"epoch_train_acc_B\": epoch_train_acc_B,\n",
    "                \"epoch_train_acc_I\": epoch_train_acc_I,\n",
    "                \"epoch_val_acc_B\": epoch_val_acc_B,\n",
    "                \"epoch_val_acc_I\": epoch_val_acc_I,\n",
    "                \"test_acc\": [test_acc_B, test_acc_I]}\n",
    "dict_name = \"BT19Icub_joint_ae_wrB_{}_wcB_{}_wrI_{}_wcI_{}_wC_{}_{}.pkl\".format(w_rB, w_cB, w_rI, w_cI, w_mse, str(kfold_number))\n",
    "pickle.dump(results_dict, open(logDir + dict_name, 'wb'))\n",
    "print(\"dump results dict to {}\".format(dict_name))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### plot the train acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert n_epochs == len(epoch_train_acc_B), \"different epoch length {} {}\".format(n_epochs, len(epoch_train_acc_B))\n",
    "fig, ax = plt.subplots(figsize=(15, 7))\n",
    "ax.plot(np.arange(n_epochs), epoch_train_acc_B, label=\"train acc B\")\n",
    "ax.set_xlabel('epoch')\n",
    "ax.set_ylabel('acc')\n",
    "ax.grid(True)\n",
    "plt.legend(loc='upper right')\n",
    "figname = logDir + model_name_B+\"_train_acc.png\"\n",
    "plt.savefig(figname)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert n_epochs == len(epoch_train_acc_I), \"different epoch length {} {}\".format(n_epochs, len(epoch_train_acc_I))\n",
    "fig, ax = plt.subplots(figsize=(15, 7))\n",
    "ax.plot(np.arange(n_epochs), epoch_train_acc_I, label=\"train acc I\")\n",
    "ax.set_xlabel('epoch')\n",
    "ax.set_ylabel('acc')\n",
    "ax.grid(True)\n",
    "plt.legend(loc='upper right')\n",
    "figname = logDir + model_name_I + \"_train_acc.png\"\n",
    "plt.savefig(figname)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
