{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "python T1_BT19_Icub_joint_ae_test.py -k 0 -c 0 -r 1\n",
    "\n",
    "Note: load pretrained model and test the accracy\n",
    "carry out the swap-classifier test\n",
    "'''\n",
    "# Import\n",
    "\n",
    "import os,sys\n",
    "CURRENT_TEST_DIR = os.getcwd()\n",
    "sys.path.append(CURRENT_TEST_DIR + \"/../new_iteration/\")\n",
    "import pickle\n",
    "import argparse\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from datetime import datetime\n",
    "\n",
    "from vrae.vrae import VRAEC\n",
    "from vrae.utils import *\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.utils import data as data2\n",
    "from torch.utils.data.dataset import random_split\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "from datetime import datetime\n",
    "from tas_utils_bs import get_trainValLoader, get_testLoader\n",
    "import plotly\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Parse argument\n",
    "# parser = argparse.ArgumentParser()\n",
    "# parser.add_argument(\"-k\", \"--kfold\", type=int, default=0, help=\"kfold_number for loading data\")\n",
    "# parser.add_argument(\"-r\", \"--reduction\", type=int, default=1, help=\"data reduction ratio for partial training\")\n",
    "# parser.add_argument(\"-c\", \"--cuda\", default=0, help=\"index of cuda gpu to use\")\n",
    "# args = parser.parse_args()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dummy class to replace argparser\n",
    "class Args:\n",
    "  kfold = 0\n",
    "  reduction = 1\n",
    "  cuda = '0'\n",
    "\n",
    "args=Args()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "load 0 kfold number, reduce data to 1 folds, put to cuda:0\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<torch._C.Generator at 0x7fc8d5461570>"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(\"load {} kfold number, reduce data to {} folds, put to cuda:{}\".format(args.kfold, args.reduction, args.cuda))\n",
    "\n",
    "# Set hyper params\n",
    "kfold_number = args.kfold\n",
    "data_reduction_ratio = args.reduction\n",
    "shuffle = True\n",
    "num_class = 20\n",
    "sequence_length_B = 400\n",
    "sequence_length_I = 75\n",
    "number_of_features_B = 19\n",
    "number_of_features_I = 60\n",
    "\n",
    "hidden_size = 90\n",
    "hidden_layer_depth = 1\n",
    "latent_length = 40\n",
    "batch_size = 32\n",
    "learning_rate = 0.0005\n",
    "n_epochs = 20\n",
    "\n",
    "learning_rate_2 = 0.01\n",
    "n_epochs_2 = 20\n",
    "\n",
    "dropout_rate = 0.2\n",
    "cuda = True # options: True, False\n",
    "print_every=30\n",
    "clip = True # options: True, False\n",
    "max_grad_norm=5\n",
    "header_B = None\n",
    "header_I = \"CNN\"\n",
    "\n",
    "w_mse = 1 # mse between latent vectors\n",
    "w_rB = 0.01 # recon for B\n",
    "w_rI = 0.01 # recon for I\n",
    "w_cB = 1 # classify for B\n",
    "w_cI = 1 # classify for I\n",
    "\n",
    "\n",
    "np.random.seed(1)\n",
    "torch.manual_seed(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load data and preprocess"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading data...\n"
     ]
    }
   ],
   "source": [
    "# Load data\n",
    "data_dir = '../../new_data_folder/'\n",
    "\n",
    "logDir = 'models_and_stat/'\n",
    "# new model\n",
    "model_name_B = 'BT19_joint_ae_wrB_{}_wcB_{}_wrI_{}_wcI_{}_wC_{}_{}'\\\n",
    "    .format(w_rB,w_cB, w_rI, w_cI, w_mse, str(kfold_number))\n",
    "model_name_I = 'IcubCNN_joint_ae_wrB_{}_wcB_{}_wrI_{}_wcI_{}_wC_{}_{}'\\\n",
    "    .format(w_rB,w_cB, w_rI, w_cI, w_mse, str(kfold_number))\n",
    "device = torch.device(\"cuda:{}\".format(args.cuda))\n",
    "print(\"Loading data...\")\n",
    "\n",
    "# manually change the test data\n",
    "kfold_number = 1\n",
    "train_loader, val_loader, train_dataset, val_dataset = get_trainValLoader(data_dir, k=kfold_number, spike_ready=False, batch_size=batch_size, shuffle=shuffle)\n",
    "test_loader, test_dataset = get_testLoader(data_dir, spike_ready=False, batch_size=batch_size, shuffle=shuffle)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/students/student6_16/anaconda3/lib/python3.7/site-packages/torch/nn/modules/rnn.py:51: UserWarning:\n",
      "\n",
      "dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.2 and num_layers=1\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "load model from\n",
      "BT19_joint_ae_wrB_0.01_wcB_1_wrI_0.01_wcI_1_wC_1_0\n",
      "IcubCNN_joint_ae_wrB_0.01_wcB_1_wrI_0.01_wcI_1_wC_1_0\n"
     ]
    }
   ],
   "source": [
    "model_B_pretrained = VRAEC(num_class=num_class,\n",
    "            sequence_length=sequence_length_B,\n",
    "            number_of_features = number_of_features_B,\n",
    "            hidden_size = hidden_size, \n",
    "            hidden_layer_depth = hidden_layer_depth,\n",
    "            latent_length = latent_length,\n",
    "            batch_size = batch_size,\n",
    "            learning_rate = learning_rate,\n",
    "            n_epochs = n_epochs,\n",
    "            dropout_rate = dropout_rate, \n",
    "            cuda = cuda,\n",
    "            print_every=print_every, \n",
    "            clip=clip, \n",
    "            max_grad_norm=max_grad_norm,\n",
    "            dload = logDir,\n",
    "            model_name=model_name_B,\n",
    "            header=header_B,\n",
    "            device = device)\n",
    "model_B_pretrained_dir = logDir+model_name_B+'.pt'\n",
    "model_B_pretrained.load_state_dict(torch.load(model_B_pretrained_dir))\n",
    "model_B_pretrained.to(device)\n",
    "model_B_pretrained.eval()\n",
    "\n",
    "model_I_pretrained = VRAEC(num_class=num_class,\n",
    "            sequence_length=sequence_length_I,\n",
    "            number_of_features = number_of_features_I,\n",
    "            hidden_size = hidden_size, \n",
    "            hidden_layer_depth = hidden_layer_depth,\n",
    "            latent_length = latent_length,\n",
    "            batch_size = batch_size,\n",
    "            learning_rate = learning_rate,\n",
    "            n_epochs = n_epochs,\n",
    "            dropout_rate = dropout_rate, \n",
    "            cuda = cuda,\n",
    "            print_every=print_every, \n",
    "            clip=clip, \n",
    "            max_grad_norm=max_grad_norm,\n",
    "            dload = logDir,\n",
    "            model_name=model_name_I,\n",
    "            header=header_I,\n",
    "            device = device)\n",
    "model_I_pretrained_dir = logDir+model_name_I+'.pt'\n",
    "model_I_pretrained.load_state_dict(torch.load(model_I_pretrained_dir))\n",
    "model_I_pretrained.to(device)\n",
    "model_I_pretrained.eval()\n",
    "\n",
    "print(\"load model from\")\n",
    "print(model_name_B)\n",
    "print(model_name_I)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model_B.is_fitted = True\n",
    "# model_I.is_fitted = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test accuracy for 1 fold 192 samples: B 0.9583333333333334, I 0.9322916666666666\n"
     ]
    }
   ],
   "source": [
    "# test the original model\n",
    "# TEST\n",
    "correct_B = 0\n",
    "correct_I = 0\n",
    "test_num = 0\n",
    "\n",
    "for i, (XI, XB,  y) in enumerate(test_loader):\n",
    "    XI, XB, y = XI.to(device), XB.to(device), y.long().to(device)\n",
    "\n",
    "    if XI.size()[0] != batch_size:\n",
    "#             print(\"batch {} size {} < {}, skip\".format(i, x.size()[0], batch_size))\n",
    "        break\n",
    "\n",
    "    test_num += XI.size(0)\n",
    "\n",
    "    # test model_B\n",
    "    x_decoded_B, latent_B, output = model_B_pretrained(XB)\n",
    "\n",
    "    # compute classification acc\n",
    "    pred = output.data.max(1, keepdim=True)[1]  # get the index of the max log-probability\n",
    "    correct_B += pred.eq(y.data.view_as(pred)).long().cpu().sum().item()\n",
    "\n",
    "    \n",
    "    # test modelI \n",
    "    x_decoded_I, latent_I, output = model_I_pretrained(XI)\n",
    "\n",
    "    # compute classification acc\n",
    "    pred = output.data.max(1, keepdim=True)[1]  # get the index of the max log-probability\n",
    "    correct_I += pred.eq(y.data.view_as(pred)).long().cpu().sum().item()\n",
    "\n",
    "test_acc_B = correct_B/test_num\n",
    "test_acc_I = correct_I/test_num\n",
    "print('Test accuracy for {} fold {} samples: B {}, I {}'.format(str(kfold_number),test_num, test_acc_B, test_acc_I))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create new models to swap classifiers\n",
    "model_B = VRAEC(num_class=num_class,\n",
    "            sequence_length=sequence_length_B,\n",
    "            number_of_features = number_of_features_B,\n",
    "            hidden_size = hidden_size, \n",
    "            hidden_layer_depth = hidden_layer_depth,\n",
    "            latent_length = latent_length,\n",
    "            batch_size = batch_size,\n",
    "            learning_rate = learning_rate,\n",
    "            n_epochs = n_epochs,\n",
    "            dropout_rate = dropout_rate, \n",
    "            cuda = cuda,\n",
    "            print_every=print_every, \n",
    "            clip=clip, \n",
    "            max_grad_norm=max_grad_norm,\n",
    "            dload = logDir,\n",
    "            model_name=model_name_B,\n",
    "            header=header_B,\n",
    "            device = device)\n",
    "\n",
    "model_I = VRAEC(num_class=num_class,\n",
    "            sequence_length=sequence_length_I,\n",
    "            number_of_features = number_of_features_I,\n",
    "            hidden_size = hidden_size, \n",
    "            hidden_layer_depth = hidden_layer_depth,\n",
    "            latent_length = latent_length,\n",
    "            batch_size = batch_size,\n",
    "            learning_rate = learning_rate,\n",
    "            n_epochs = n_epochs,\n",
    "            dropout_rate = dropout_rate, \n",
    "            cuda = cuda,\n",
    "            print_every=print_every, \n",
    "            clip=clip, \n",
    "            max_grad_norm=max_grad_norm,\n",
    "            dload = logDir,\n",
    "            model_name=model_name_I,\n",
    "            header=header_I,\n",
    "            device = device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "VRAE(n_epochs=20,batch_size=32,cuda=True)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# interchange classifer\n",
    "classifier_keys = ['classifier.0.weight', 'classifier.0.bias']\n",
    "classifier_dict_B = {k: v for k, v in model_B_pretrained.state_dict().items() if k in classifier_keys}\n",
    "classifier_dict_I = {k: v for k, v in model_I_pretrained.state_dict().items() if k in classifier_keys}\n",
    "ae_dict_B = {k: v for k, v in model_B_pretrained.state_dict().items() if k not in classifier_keys}\n",
    "ae_dict_I = {k: v for k, v in model_I_pretrained.state_dict().items() if k not in classifier_keys}\n",
    "\n",
    "newB_dict = model_B.state_dict()\n",
    "newI_dict = model_I.state_dict()\n",
    "newB_dict.update(ae_dict_B)\n",
    "newI_dict.update(ae_dict_I)\n",
    "# overwrite classifer for new models\n",
    "newB_dict.update(classifier_dict_I)\n",
    "newI_dict.update(classifier_dict_B)\n",
    "# load the new state_dict\n",
    "model_B.load_state_dict(newB_dict)\n",
    "model_I.load_state_dict(newI_dict)\n",
    "# print(\"classifier_dict_new_B\")\n",
    "# print(classifier_dict_new_B)\n",
    "model_B.to(device)\n",
    "model_I.to(device)\n",
    "model_B.eval()\n",
    "model_I.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test accuracy after swapping for 1 fold 192 samples: B 0.953125, I 0.9322916666666666\n"
     ]
    }
   ],
   "source": [
    "# TEST\n",
    "correct_B = 0\n",
    "correct_I = 0\n",
    "test_num = 0\n",
    "\n",
    "for i, (XI, XB,  y) in enumerate(test_loader):\n",
    "    XI, XB, y = XI.to(device), XB.to(device), y.long().to(device)\n",
    "\n",
    "    if XI.size()[0] != batch_size:\n",
    "#             print(\"batch {} size {} < {}, skip\".format(i, x.size()[0], batch_size))\n",
    "        break\n",
    "\n",
    "    test_num += XI.size(0)\n",
    "\n",
    "    # test model_B\n",
    "    x_decoded_B, latent_B, output = model_B(XB)\n",
    "\n",
    "    # compute classification acc\n",
    "    pred = output.data.max(1, keepdim=True)[1]  # get the index of the max log-probability\n",
    "    correct_B += pred.eq(y.data.view_as(pred)).long().cpu().sum().item()\n",
    "\n",
    "    \n",
    "    # test modelI \n",
    "    x_decoded_I, latent_I, output = model_I(XI)\n",
    "\n",
    "    # compute classification acc\n",
    "    pred = output.data.max(1, keepdim=True)[1]  # get the index of the max log-probability\n",
    "    correct_I += pred.eq(y.data.view_as(pred)).long().cpu().sum().item()\n",
    "\n",
    "test_acc_B = correct_B/test_num\n",
    "test_acc_I = correct_I/test_num\n",
    "print('Test accuracy after swapping for {} fold {} samples: B {}, I {}'.format(str(kfold_number),test_num, test_acc_B, test_acc_I))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot_clustering(z_runs[0], y_val[0], engine='matplotlib', download = True, folder_name='figures', filefix='_BT19_joint_{}'.format(n_epochs))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot_clustering(z_runs[1], y_val[1], engine='matplotlib', download = True, folder_name='figures', filefix='_Icub_joint_{}'.format(n_epochs))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
